{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install tables\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e280f03a3113c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('processed_data.pkl')\n",
    "training_df = df[df[\"train\"]]\n",
    "testing_df = df[df[\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc0d3e00d125c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROMT_DIR_PATH=Path(\"prompt-variations\")\n",
    "\n",
    "PROMT_PATHS = sorted(PROMT_DIR_PATH.glob(\"v*.txt\"), key=lambda f:int(f.name.strip(\"v.txt\")))\n",
    "SYSTEM_PROMPT = [f.read_text() for f in PROMT_PATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf84439",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "PERPLEXITY_API_KEY = os.getenv(\"PPLX_KEY\")\n",
    "VALID_MODELS = ['codellama-34b-instruct', 'llama-2-70b-chat', 'mistral-7b-instruct', 'mixtral-8x7b-instruct',\n",
    "                'pplx-7b-chat', 'pplx-70b-chat']\n",
    "\n",
    "\n",
    "def call_api(model: str, promt: str, text: str) -> requests.Response:\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": promt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\"\n",
    "    }\n",
    "\n",
    "    return requests.post(\"https://api.perplexity.ai/chat/completions\", json=payload, headers=headers)\n",
    "\n",
    "\n",
    "def run_model(prompt_id, model, sample_size, print_results=False):\n",
    "    if model not in VALID_MODELS:\n",
    "        raise ValueError(f'Invalid model {model}. Valid options are {VALID_MODELS}')\n",
    "\n",
    "    sample_df = testing_df.sample(n=sample_size)\n",
    "\n",
    "    results = []\n",
    "    start_time = datetime.datetime.now()\n",
    "    for i, (row_index, row) in enumerate(sample_df.iterrows()):\n",
    "        total = sample_df[\"text\"].count()\n",
    "        counter = i + 1\n",
    "        elapsed = datetime.datetime.now() - start_time\n",
    "        percentage = counter / total\n",
    "        s_per_gen=elapsed / counter\n",
    "        print(f'[{elapsed}<{s_per_gen * (total - counter)}, {s_per_gen}s/generations] '\n",
    "              f'{model} - promt {prompt_id}: {counter}/{total} | {percentage * 100:.2f}%')\n",
    "        backoff_time = 10\n",
    "        while True:\n",
    "            response = call_api(model, SYSTEM_PROMPT[prompt_id], row[\"text\"])\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "            print(\n",
    "                f\"{model} (Promt {prompt_id}): Error {response.status_code} => sleeping for {backoff_time}s: {response.text}\")\n",
    "            time.sleep(backoff_time)\n",
    "            backoff_time = min(int(1.2 * backoff_time), 60)\n",
    "        data = json.loads(response.text)\n",
    "        answer = data['choices'][0]['message']['content']\n",
    "        results.append({\n",
    "            'prompt_id': prompt_id,\n",
    "            'model': model,\n",
    "            'sample_size': sample_size,\n",
    "            \"text\": row[\"text\"],\n",
    "            \"answer\": answer,\n",
    "            \"labeled_hateful\": row[\"hate\"]\n",
    "        })\n",
    "        if print_results:\n",
    "            print(row[\"text\"])\n",
    "            if lines := [line for line in answer.split(\"\\n\") if \"hate_speech_probability\" in line]:\n",
    "                print(lines)\n",
    "            else:\n",
    "                print(answer)\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "total_samples = len(testing_df)\n",
    "all_runs = pd.read_pickle(\"all_runs.pkl\")\n",
    "\n",
    "CONFIG = [\n",
    "    (5, 'mistral-7b-instruct', total_samples),\n",
    "    (6, 'mistral-7b-instruct', total_samples),\n",
    "    (7, 'mistral-7b-instruct', total_samples),\n",
    "    (8, 'mistral-7b-instruct', total_samples),\n",
    "]\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for run in executor.map(lambda config: run_model(*config), CONFIG):\n",
    "        all_runs = pd.concat([all_runs, run])\n",
    "        pd.DataFrame(all_runs).to_pickle(\"all_runs.pkl\")"
   ],
   "metadata": {},
   "id": "05aa5115",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_runs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd355e8d7d370a48",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
