{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas numpy plotly kaleido"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_pickle('all_runs.pkl')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"'\", \"\\\"\")\n",
    "    text = re.sub(\"\\\\\\([_\\[\\]{}])\", r\"\\1\", text)\n",
    "    for deletion in [\"```json\", r\"\\n\"]:\n",
    "        text = text.replace(deletion, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"answer\"] = df[\"answer\"].apply(clean_text)\n",
    "\n",
    "\n",
    "def extract_str(json_string: str, key: str):\n",
    "    pattern = f'{key}\"?: (\"[^\"]*\")(?:,|\\n)'\n",
    "    match = re.search(pattern, json_string)\n",
    "    if not match:\n",
    "        return None\n",
    "    return match.group(1).strip().strip('\"') if match else None\n",
    "\n",
    "\n",
    "def extract_float(json_string: str, key: str):\n",
    "    pattern = f'{key}\"?: (.*?)(?:,|\\n)'\n",
    "    match = re.search(pattern, json_string)\n",
    "    if not match:\n",
    "        return None\n",
    "    value = match.group(1).strip()\n",
    "    try:\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_list(json_string: str, key: str):\n",
    "    pattern = f'{key}\"?: ?(\\[[^\\]]*\\])(?:,|\\n)'\n",
    "    match = re.search(pattern, json_string)\n",
    "    if not match:\n",
    "        return None\n",
    "    value = match.group(1).strip()\n",
    "    try:\n",
    "        return json.loads(value)\n",
    "    except Exception:\n",
    "        return value\n",
    "\n",
    "\n",
    "def extract_dict(json_string: str, key: str):\n",
    "    pattern = key + r'\"?: ?({[^}]*})'\n",
    "    match = re.search(pattern, json_string)\n",
    "    if not match:\n",
    "        return None\n",
    "    value = match.group(1).strip()\n",
    "    try:\n",
    "        return json.loads(value)\n",
    "    except Exception:\n",
    "        return value\n",
    "\n",
    "\n",
    "list_keys = [\n",
    "    \"relevant_facts_from_the_cultural_context\",\n",
    "    \"contained_explicit_racist_vocabulary\",\n",
    "    \"contained_explicit_sexist_vocabulary\",\n",
    "    \"contained_explicit_toxic_vocabulary\",\n",
    "]\n",
    "\n",
    "for key in list_keys:\n",
    "    df[key] = df['answer'].apply(lambda x: extract_list(x, key))\n",
    "\n",
    "dict_keys = [\n",
    "    \"relevant_word_definitions\"\n",
    "]\n",
    "\n",
    "for key in dict_keys:\n",
    "    df[key] = df['answer'].apply(lambda x: extract_dict(x, key))\n",
    "\n",
    "str_keys = [\n",
    "    \"argument_for_hate_speech\",\n",
    "    \"argument_against_hate_speech\",\n",
    "]\n",
    "\n",
    "for key in str_keys:\n",
    "    df[key] = df['answer'].apply(lambda x: extract_str(x, key))\n",
    "\n",
    "float_keys = [\n",
    "    \"likelihood_of_presence_of_implicit_hate_speech\",\n",
    "    \"likelihood_of_presence_of_explicit_hate_speech\",\n",
    "    \"likelihood_of_ad_hominin_attack\",\n",
    "    \"likelihood_of_minority_attack\",\n",
    "    \"likelihood_of_takedown_on_social_media\",\n",
    "    \"hate_speech_probability\",\n",
    "]\n",
    "\n",
    "for key in float_keys:\n",
    "    df[key] = df['answer'].apply(lambda x: extract_float(x, key))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[df['answer'].apply(lambda x: \"hate_speech_probability\" not in str(x))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values for each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "df[\"bin\"] = pd.cut(df['hate_speech_probability'] * 100, bins=np.arange(0, 105, 5), include_lowest=True)\n",
    "df['bin'] = df[\"bin\"].apply(lambda bin: str(bin))\n",
    "\n",
    "df_grouped = df.groupby([\"bin\", \"model\", \"prompt_id\", \"labeled_hateful\"]).count().reset_index()\n",
    "fig = px.box(df_grouped, x=\"bin\", y=\"text\", color=\"labeled_hateful\", hover_name=\"model\")\n",
    "fig.update_layout(xaxis=dict(title=\"our hate speech probability\"), yaxis=dict(title=\"number of scores by model\"))\n",
    "fig.write_image(\"labeled_all_runs_all_models.png\", width=2000, height=600)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "df[\"bin\"] = pd.cut(df['hate_speech_probability'] * 100, bins=np.arange(0, 110, 10), include_lowest=True)\n",
    "df['bin'] = df[\"bin\"].apply(lambda bin: str(bin))\n",
    "\n",
    "true_df = df.query(\"not labeled_hateful\")\n",
    "df_grouped = true_df.groupby([\"bin\", \"model\", \"prompt_id\"]).count().reset_index()\n",
    "fig = px.scatter(df_grouped, x=\"bin\", y=\"text\", color=\"model\", hover_data=\"prompt_id\", hover_name=\"model\",\n",
    "                 title=\"Hate Speech Probability (only non-hatefull, our probability should be 0)\")\n",
    "fig.update_layout(xaxis=dict(title=\"our hate speech probability\"), yaxis=dict(title=\"number of scores by model\"))\n",
    "fig.write_image(\"hate_prob_per_model_only-non-hatefull.png\", width=2000, height=600)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "df[\"bin\"] = pd.cut(df['hate_speech_probability'] * 100, bins=np.arange(0, 110, 10), include_lowest=True)\n",
    "df['bin'] = df[\"bin\"].apply(lambda bin: str(bin))\n",
    "\n",
    "true_df = df.query(\"labeled_hateful\")\n",
    "df_grouped = true_df.groupby([\"bin\", \"model\", \"prompt_id\"]).count().reset_index()\n",
    "fig = px.scatter(df_grouped, x=\"bin\", y=\"text\", color=\"model\", hover_data=\"prompt_id\", hover_name=\"model\", title=\"Hate Speech Probability (only hatefull, our probability should be 1)\")\n",
    "fig.update_layout(xaxis=dict(title=\"our hate speech probability\"), yaxis=dict(title=\"number of scores by model\"))\n",
    "fig.write_image(\"hate_prob_per_model_only-hatefull.png\", width=2000, height=600)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def draw_roc_curve(fpr, tpr, label):\n",
    "    fig = px.line(\n",
    "        x=fpr, y=tpr,\n",
    "        title=f'ROC Curve {label} (AUC={auc(fpr, tpr):.4f})',\n",
    "        labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "        width=700, height=500\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=0, y1=1\n",
    "    )\n",
    "    fig.update_layout(title_x=0.5)\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig.update_xaxes(constrain='domain')\n",
    "    fig.write_image(label.replace(\" - \", \"_\")+\"_roc_curve.png\")\n",
    "    fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss, brier_score_loss, f1_score, accuracy_score, roc_curve\n",
    "\n",
    "metrics = []\n",
    "classified_metrics = []\n",
    "for model in df[\"model\"].unique():\n",
    "    if \"mistral\" not in model:\n",
    "        continue\n",
    "    # Assuming y_true is your array of true labels and y_pred is your array of predicted probabilities\n",
    "    no_nans = df[df[\"hate_speech_probability\"].notna()]\n",
    "    #filtered_by_promt = no_nans[no_nans[\"prompt_id\"].apply(lambda prompt_id:not (2<=prompt_id<=4))]\n",
    "    filtered_by_model = no_nans[no_nans[\"model\"] == model]\n",
    "    \n",
    "    for prompt_id in filtered_by_model[\"prompt_id\"].unique():\n",
    "        model_prompt_specific = filtered_by_model[filtered_by_model[\"prompt_id\"] == prompt_id]\n",
    "        y_true = model_prompt_specific['labeled_hateful']\n",
    "        y_pred = model_prompt_specific['hate_speech_probability']\n",
    "\n",
    "        print(f\" {model} - {prompt_id} \".center(46, '-'))\n",
    "        label=f\"{model} - {prompt_id}\"\n",
    "        metrics.append({\"color\": label, \"metric\": \"roc_auc_score\", \"value\": roc_auc_score(y_true, y_pred)})\n",
    "        metrics.append({\"color\": label, \"metric\": \"log_loss\", \"value\": log_loss(y_true, y_pred)})\n",
    "        metrics.append({\"color\": label, \"metric\": \"brier_score_loss\", \"value\": brier_score_loss(y_true, y_pred)})\n",
    "\n",
    "        for i in range(100):\n",
    "            cuttoff = i / 100\n",
    "            classification = y_pred.apply(lambda x: x > cuttoff)\n",
    "            classified_metrics.append({\"x\": cuttoff,\n",
    "                                       \"color\": label,\n",
    "                                       \"f1\": f1_score(y_true, classification),\n",
    "                                       \"accuracy\": accuracy_score(y_true, classification)})\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        draw_roc_curve(fpr,tpr, label)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = px.bar(metrics, x=\"value\", y=\"color\", barmode=\"relative\", color=\"color\",\n",
    "             facet_col=\"metric\", facet_col_spacing=0.05, labels=\"color\")\n",
    "for i in range(1, 4):\n",
    "    fig.update_layout({f\"xaxis{str(i).replace('0', '')}\": dict(title=\"model - promt variation\"),\n",
    "                       f\"yaxis{str(i).replace('0', '')}\": dict(title=\"metric value\")})\n",
    "fig.update_xaxes(matches=None)\n",
    "\n",
    "\n",
    "fig.write_image(\"model_promt_variations.png\", width=2000, height=600)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "classified_stats = pd.DataFrame(classified_metrics)\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"F1 scores\", \"Accuracy scores\"))\n",
    "\n",
    "for i, label in enumerate(classified_stats[\"color\"].unique()):\n",
    "    if \"- 0\" not in label and \"- 6\" not in label:\n",
    "        continue\n",
    "    filtered = classified_stats[classified_stats['color'] == label]\n",
    "    color = plotly.colors.DEFAULT_PLOTLY_COLORS[i % len(plotly.colors.DEFAULT_PLOTLY_COLORS)]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=filtered[\"x\"], y=filtered[\"f1\"], text=label, legendgroup=label, legendgrouptitle=dict(text=label),\n",
    "                   mode='markers', name=\"f1 score\", marker=go.scatter.Marker(color=color, symbol=\"x-thin-open\")),\n",
    "        row=1, col=1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=filtered[\"x\"], y=filtered[\"accuracy\"], legendgroup=label, legendgrouptitle=dict(text=label),\n",
    "                   text=label, mode='markers', name=\"accuracy\",\n",
    "                   marker=go.scatter.Marker(color=color, symbol=\"cross-thin-open\")),\n",
    "        row=1, col=2)\n",
    "\n",
    "fig.update_layout(xaxis=dict(title=\"by cutoff\"), xaxis2=dict(title=\"by cutoff\"), yaxis=dict(title=\"f1 score\"),\n",
    "                  yaxis2=dict(title=\"accuracy score\"))\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"top\",\n",
    "        y=-0.5,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    ))\n",
    "fig.update_yaxes(range=[0, 1])\n",
    "\n",
    "fig.write_image(\"fi_accuracy.png\", width=2000, height=700)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df = df[[\"prompt_id\", \"model\", \"hate_speech_probability\", \"labeled_hateful\", \"text\"]]\n",
    "# False Negatives\n",
    "false_negatives = df[\n",
    "    (df['model'] == 'mistral-7b-instruct') &\n",
    "    (df['prompt_id'] == 6) &\n",
    "    (df['labeled_hateful'] == True) &\n",
    "    (df['hate_speech_probability'] < 0.2)\n",
    "]\n",
    "\n",
    "# False Positives\n",
    "false_positives = df[\n",
    "    (df['model'] == 'mistral-7b-instruct') &\n",
    "    (df['prompt_id'] == 6) &\n",
    "    (df['labeled_hateful'] == False) &\n",
    "    (df['hate_speech_probability'] >= 0.5)\n",
    "]\n",
    "\n",
    "false_negatives = false_negatives.sort_values(by=['text'], key=lambda x: x.str.len())\n",
    "false_positives = false_positives.sort_values(by=['text'], key=lambda x: x.str.len())\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(len(false_negatives))\n",
    "print(len(false_positives))\n",
    "\n",
    "print(false_negatives['text'].values[0])\n",
    "false_positives"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
