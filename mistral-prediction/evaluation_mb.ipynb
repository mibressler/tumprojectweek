{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas numpy plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_pickle('all_runs.pkl')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"'\", \"\\\"\")\n",
    "    text = re.sub(\"\\\\\\([_\\[\\]{}])\", r\"\\1\", text)\n",
    "    for deletion in [\"```json\", r\"\\n\"]:\n",
    "        text = text.replace(deletion, \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"answer\"] = df[\"answer\"].apply(clean_text)\n",
    "\n",
    "\n",
    "def extract_str(json_string: str, key: str):\n",
    "    pattern = f'{key}\"?: (\"[^\"]*\")(?:,|\\n)'\n",
    "    match = re.search(pattern, json_string)\n",
    "    if not match:\n",
    "        return None\n",
    "    return match.group(1).strip().strip('\"') if match else None\n",
    "\n",
    "\n",
    "def extract_float(json_string: str, key: str):\n",
    "    pattern = f'{key}\"?: (.*?)(?:,|\\n)'\n",
    "    match = re.search(pattern, json_string)\n",
    "    if not match:\n",
    "        return None\n",
    "    value = match.group(1).strip()\n",
    "    try:\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_list(json_string: str, key: str):\n",
    "    pattern = f'{key}\"?: ?(\\[[^\\]]*\\])(?:,|\\n)'\n",
    "    match = re.search(pattern, json_string)\n",
    "    if not match:\n",
    "        return None\n",
    "    value = match.group(1).strip()\n",
    "    try:\n",
    "        return json.loads(value)\n",
    "    except Exception:\n",
    "        return value\n",
    "\n",
    "\n",
    "def extract_dict(json_string: str, key: str):\n",
    "    pattern = key + r'\"?: ?({[^}]*})'\n",
    "    match = re.search(pattern, json_string)\n",
    "    if not match:\n",
    "        return None\n",
    "    value = match.group(1).strip()\n",
    "    try:\n",
    "        return json.loads(value)\n",
    "    except Exception:\n",
    "        return value\n",
    "\n",
    "\n",
    "list_keys = [\n",
    "    \"relevant_facts_from_the_cultural_context\",\n",
    "    \"contained_explicit_racist_vocabulary\",\n",
    "    \"contained_explicit_sexist_vocabulary\",\n",
    "    \"contained_explicit_toxic_vocabulary\",\n",
    "]\n",
    "\n",
    "for key in list_keys:\n",
    "    df[key] = df['answer'].apply(lambda x: extract_list(x, key))\n",
    "\n",
    "dict_keys = [\n",
    "    \"relevant_word_definitions\"\n",
    "]\n",
    "\n",
    "for key in dict_keys:\n",
    "    df[key] = df['answer'].apply(lambda x: extract_dict(x, key))\n",
    "\n",
    "str_keys = [\n",
    "    \"argument_for_hate_speech\",\n",
    "    \"argument_against_hate_speech\",\n",
    "]\n",
    "\n",
    "for key in str_keys:\n",
    "    df[key] = df['answer'].apply(lambda x: extract_str(x, key))\n",
    "\n",
    "float_keys = [\n",
    "    \"likelihood_of_presence_of_implicit_hate_speech\",\n",
    "    \"likelihood_of_presence_of_explicit_hate_speech\",\n",
    "    \"likelihood_of_ad_hominin_attack\",\n",
    "    \"likelihood_of_minority_attack\",\n",
    "    \"likelihood_of_takedown_on_social_media\",\n",
    "    \"hate_speech_probability\",\n",
    "]\n",
    "\n",
    "for key in float_keys:\n",
    "    df[key] = df['answer'].apply(lambda x: extract_float(x, key))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df[df['answer'].apply(lambda x: \"hate_speech_probability\" not in str(x))]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values for each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "df[\"bin\"] = pd.cut(df['hate_speech_probability'] * 100, bins=np.arange(0, 105, 5), include_lowest=True)\n",
    "df['bin'] = df[\"bin\"].apply(lambda bin: str(bin))\n",
    "\n",
    "df_grouped = df.groupby([\"bin\", \"model\", \"prompt_id\", \"labeled_hateful\"]).count().reset_index()\n",
    "fig = px.box(df_grouped, x=\"bin\", y=\"text\", color=\"labeled_hateful\", hover_name=\"model\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "df[\"bin\"] = pd.cut(df['hate_speech_probability'] * 100, bins=np.arange(0, 110, 10), include_lowest=True)\n",
    "df['bin'] = df[\"bin\"].apply(lambda bin: str(bin))\n",
    "\n",
    "true_df = df.query(\"not labeled_hateful\")\n",
    "df_grouped = true_df.groupby([\"bin\", \"model\", \"prompt_id\"]).count().reset_index()\n",
    "fig = px.scatter(df_grouped, x=\"bin\", y=\"text\", color=\"model\", hover_data=\"prompt_id\", hover_name=\"model\",\n",
    "                 title=\"Hate Speech Probability (only non-hatefull, our probability should be 0)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "df[\"bin\"] = pd.cut(df['hate_speech_probability'] * 100, bins=np.arange(0, 110, 10), include_lowest=True)\n",
    "df['bin'] = df[\"bin\"].apply(lambda bin: str(bin))\n",
    "\n",
    "true_df = df.query(\"labeled_hateful\")\n",
    "df_grouped = true_df.groupby([\"bin\", \"model\", \"prompt_id\"]).count().reset_index()\n",
    "fig = px.scatter(df_grouped, x=\"bin\", y=\"text\", color=\"model\", hover_data=\"prompt_id\", hover_name=\"model\",\n",
    "                 title=\"Hate Speech Probability (only hatefull, our probability should be 1)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss, brier_score_loss, f1_score, accuracy_score\n",
    "\n",
    "metrics = []\n",
    "classified_metrics = []\n",
    "for model in df[\"model\"].unique():\n",
    "    if \"mistral\" not in model:\n",
    "        continue\n",
    "    # Assuming y_true is your array of true labels and y_pred is your array of predicted probabilities\n",
    "    no_nans = df[df[\"hate_speech_probability\"].notna()]\n",
    "    filtered_by_promt = no_nans[no_nans[\"prompt_id\"].apply(lambda prompt_id:not (2<=prompt_id<=4))]\n",
    "    filtered_by_model = filtered_by_promt[filtered_by_promt[\"model\"] == model]\n",
    "    \n",
    "    for prompt_id in filtered_by_model[\"prompt_id\"].unique():\n",
    "        model_prompt_specific = filtered_by_model[filtered_by_model[\"prompt_id\"] == prompt_id]\n",
    "        y_true = model_prompt_specific['labeled_hateful']\n",
    "        y_pred = model_prompt_specific['hate_speech_probability']\n",
    "\n",
    "        print(f\" {model} - {prompt_id} \".center(46, '-'))\n",
    "        metrics.append(\n",
    "            {\"color\": f\"{model} - {prompt_id}\", \"metric\": \"roc_auc_score\", \"value\": roc_auc_score(y_true, y_pred)})\n",
    "        metrics.append({\"color\": f\"{model} - {prompt_id}\", \"metric\": \"log_loss\", \"value\": log_loss(y_true, y_pred)})\n",
    "        metrics.append({\"color\": f\"{model} - {prompt_id}\", \"metric\": \"brier_score_loss\",\n",
    "                        \"value\": brier_score_loss(y_true, y_pred)})\n",
    "\n",
    "        for i in range(100):\n",
    "            cuttoff = i / 100\n",
    "            classification = y_pred.apply(lambda x: x > cuttoff)\n",
    "            classified_metrics.append({\"x\": cuttoff,\n",
    "                                       \"color\": f\"{model} - {prompt_id}\",\n",
    "                                       \"f1\": f1_score(y_true, classification),\n",
    "                                       \"accuracy\": accuracy_score(y_true, classification)})"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = px.bar(metrics, x=\"color\", y=\"value\", barmode=\"relative\", color=\"color\",\n",
    "             facet_col=\"metric\", facet_col_spacing=0.05, labels=\"color\", height=700, width=800\n",
    "\n",
    "             )\n",
    "for i in range(1, 4):\n",
    "    fig.update_layout({f\"xaxis{str(i).replace('0', '')}\": dict(title=\"model - promt variation\"),\n",
    "                       f\"yaxis{str(i).replace('0', '')}\": dict(title=\"metric value\")})\n",
    "fig.update_legends()\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"top\",\n",
    "        y=-0.5,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5\n",
    "    ))\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "classified_stats = pd.DataFrame(classified_metrics)\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"F1 scores\", \"Accuracy scores\"))\n",
    "\n",
    "for i, label in enumerate(classified_stats[\"color\"].unique()):\n",
    "    filtered = classified_stats[classified_stats['color'] == label]\n",
    "    color = plotly.colors.DEFAULT_PLOTLY_COLORS[i]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=filtered[\"x\"], y=filtered[\"f1\"], text=label, legendgroup=label, legendgrouptitle=dict(text=label),\n",
    "                   mode='markers', name=\"f1 score\", marker=go.scatter.Marker(color=color, symbol=\"x-thin-open\")),\n",
    "        row=1, col=1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=filtered[\"x\"], y=filtered[\"accuracy\"], legendgroup=label, legendgrouptitle=dict(text=label),\n",
    "                   text=label, mode='markers', name=\"accuracy\",\n",
    "                   marker=go.scatter.Marker(color=color, symbol=\"cross-thin-open\")),\n",
    "        row=1, col=2)\n",
    "\n",
    "fig.update_layout(xaxis=dict(title=\"by cutoff\"), xaxis2=dict(title=\"by cutoff\"), yaxis=dict(title=\"f1 score\"),\n",
    "                  yaxis2=dict(title=\"accuracy score\"))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
