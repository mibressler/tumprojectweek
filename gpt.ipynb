{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-11T04:01:29.325377971Z",
     "start_time": "2024-01-11T04:01:20.538849213Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.11/site-packages (23.3.2)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (2.1.4)\r\n",
      "Requirement already satisfied: tables in ./venv/lib/python3.11/site-packages (3.9.2)\r\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in ./venv/lib/python3.11/site-packages (from pandas) (1.26.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./venv/lib/python3.11/site-packages (from pandas) (2023.4)\r\n",
      "Requirement already satisfied: numexpr>=2.6.2 in ./venv/lib/python3.11/site-packages (from tables) (2.8.8)\r\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from tables) (23.2)\r\n",
      "Requirement already satisfied: py-cpuinfo in ./venv/lib/python3.11/site-packages (from tables) (9.0.0)\r\n",
      "Requirement already satisfied: blosc2>=2.3.0 in ./venv/lib/python3.11/site-packages (from tables) (2.4.0)\r\n",
      "Requirement already satisfied: ndindex>=1.4 in ./venv/lib/python3.11/site-packages (from blosc2>=2.3.0->tables) (1.7)\r\n",
      "Requirement already satisfied: msgpack in ./venv/lib/python3.11/site-packages (from blosc2>=2.3.0->tables) (1.0.7)\r\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/lib/python3.11/site-packages (4.36.2)\r\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.11/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.11/site-packages (0.16.2)\r\n",
      "Requirement already satisfied: torchaudio in ./venv/lib/python3.11/site-packages (2.1.2)\r\n",
      "Requirement already satisfied: peft in ./venv/lib/python3.11/site-packages (0.7.1)\r\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from transformers) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./venv/lib/python3.11/site-packages (from transformers) (0.20.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from transformers) (1.26.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.11/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./venv/lib/python3.11/site-packages (from transformers) (0.15.0)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./venv/lib/python3.11/site-packages (from transformers) (0.4.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.11/site-packages (from transformers) (4.66.1)\r\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.11/site-packages (from torch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.11/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.11/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.11/site-packages (from torch) (2023.12.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./venv/lib/python3.11/site-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./venv/lib/python3.11/site-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./venv/lib/python3.11/site-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./venv/lib/python3.11/site-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./venv/lib/python3.11/site-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./venv/lib/python3.11/site-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./venv/lib/python3.11/site-packages (from torch) (2.18.1)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./venv/lib/python3.11/site-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.1.0 in ./venv/lib/python3.11/site-packages (from torch) (2.1.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./venv/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.11/site-packages (from torchvision) (10.2.0)\r\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.11/site-packages (from peft) (5.9.7)\r\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./venv/lib/python3.11/site-packages (from peft) (0.26.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./venv/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available()=True\ttorch.cuda.device_count()=1\ttorch.version=<module 'torch.version' from '/home/frank/dev/uni/tumprojectweek/venv/lib/python3.11/site-packages/torch/version.py'>\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip \n",
    "import datetime\n",
    "!pip install pandas tables\n",
    "import pandas as pd\n",
    "!pip install transformers torch torchvision torchaudio peft\n",
    "!pip -qqq install bitsandbytes accelerate\n",
    "import torch\n",
    "\n",
    "print(f\"{torch.cuda.is_available()=}\\t{torch.cuda.device_count()=}\\t{torch.version=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f4e280f03a3113c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T04:01:29.325612691Z",
     "start_time": "2024-01-11T04:01:29.314007703Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('processed_data.pkl')\n",
    "training_df = df[df[\"train\"]]\n",
    "testing_df = df[df[\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3dc0d3e00d125c3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T04:01:29.325750282Z",
     "start_time": "2024-01-11T04:01:29.314128043Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROMT_DIR_PATH = Path(\"mistral-prediction\") / \"prompt-variations\"\n",
    "\n",
    "PROMT_PATHS = sorted(PROMT_DIR_PATH.glob(\"v*.txt\"), key=lambda f: int(f.name.strip(\"v.txt\")))\n",
    "SYSTEM_PROMPT = [f.read_text(encoding=\"utf-8\") for f in PROMT_PATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cf84439",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-11T04:01:34.720617951Z",
     "start_time": "2024-01-11T04:01:29.314201183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08fd5fdb0f2440e48a77359e3c966163"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_path = \"mibressler/tumproject\"\n",
    "token = \"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\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=nf4_config,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype='auto',\n",
    "    token=token,\n",
    ").eval()\n",
    "\n",
    "def generate_response(system_promt: str, text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": system_promt + \"\\nText to evaluate: \\\"\" + text + \"\\\"\"},\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True,\n",
    "                                              return_tensors='pt')\n",
    "    output_ids = model.generate(input_ids=input_ids.to('cuda'), max_new_tokens=1024)\n",
    "    return tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def run_model(prompt_id: int, model: str):\n",
    "    results = []\n",
    "    start_time = datetime.datetime.now()\n",
    "    for i, (row_index, row) in enumerate(testing_df.iterrows()):\n",
    "        total = testing_df[\"text\"].count()\n",
    "        counter = i + 1\n",
    "        elapsed = datetime.datetime.now() - start_time\n",
    "        percentage = counter / total\n",
    "        s_per_gen = elapsed / counter\n",
    "        print(f'[{elapsed}<{s_per_gen * (total - counter)}, {s_per_gen}s/generations] '\n",
    "              f'{model} - promt {prompt_id}: {counter}/{total} | {percentage * 100:.2f}%')\n",
    "        answer = generate_response(SYSTEM_PROMPT[prompt_id], row[\"text\"])\n",
    "        results.append({\n",
    "            'prompt_id': prompt_id,\n",
    "            'model': model,\n",
    "            'sample_size': total,\n",
    "            \"text\": row[\"text\"],\n",
    "            \"answer\": answer,\n",
    "            \"labeled_hateful\": row[\"hate\"]\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0:00:00.000374<0:00:00.178398, 0:00:00.000374s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 1/478 | 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/dev/uni/tumprojectweek/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0:00:15.565736<1:01:44.645168, 0:00:07.782868s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 2/478 | 0.42%\n",
      "[0:00:29.952371<1:19:02.458900, 0:00:09.984124s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 3/478 | 0.63%\n",
      "[0:00:40.731126<1:20:26.638668, 0:00:10.182782s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 4/478 | 0.84%\n",
      "[0:00:51.645460<1:21:25.660516, 0:00:10.329092s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 5/478 | 1.05%\n",
      "[0:01:03.174882<1:22:49.757384, 0:00:10.529147s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 6/478 | 1.26%\n",
      "[0:01:15.841276<1:25:03.034428, 0:00:10.834468s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 7/478 | 1.46%\n",
      "[0:01:28.800261<1:26:57.015510, 0:00:11.100033s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 8/478 | 1.67%\n",
      "[0:01:40.251519<1:27:04.218202, 0:00:11.139058s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 9/478 | 1.88%\n",
      "[0:01:53.610209<1:28:36.957828, 0:00:11.361021s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 10/478 | 2.09%\n",
      "[0:02:06.496631<1:29:30.357098, 0:00:11.499694s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 11/478 | 2.30%\n",
      "[0:02:18.752197<1:29:48.210278, 0:00:11.562683s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 12/478 | 2.51%\n",
      "[0:02:33.924978<1:31:45.778095, 0:00:11.840383s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 13/478 | 2.72%\n",
      "[0:02:45.798927<1:31:35.049920, 0:00:11.842780s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 14/478 | 2.93%\n",
      "[0:02:58.544104<1:31:51.061220, 0:00:11.902940s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 15/478 | 3.14%\n",
      "[0:03:15.793890<1:34:13.548516, 0:00:12.237118s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 16/478 | 3.35%\n",
      "[0:03:29.832631<1:34:50.167256, 0:00:12.343096s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 17/478 | 3.56%\n",
      "[0:03:44.165489<1:35:28.673480, 0:00:12.453638s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 18/478 | 3.77%\n",
      "[0:03:57.695288<1:35:42.217602, 0:00:12.510278s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 19/478 | 3.97%\n",
      "[0:04:13.392322<1:36:42.684128, 0:00:12.669616s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 20/478 | 4.18%\n",
      "[0:04:26.100931<1:36:30.863161, 0:00:12.671473s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 21/478 | 4.39%\n",
      "[0:04:41.735014<1:37:19.598472, 0:00:12.806137s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 22/478 | 4.60%\n",
      "[0:04:55.646917<1:37:28.667370, 0:00:12.854214s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 23/478 | 4.81%\n",
      "[0:05:07.669116<1:37:00.073884, 0:00:12.819546s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 24/478 | 5.02%\n",
      "[0:05:21.616965<1:37:07.699587, 0:00:12.864679s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 25/478 | 5.23%\n",
      "[0:05:32.252467<1:36:16.081332, 0:00:12.778941s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 26/478 | 5.44%\n",
      "[0:05:45.351281<1:36:08.645388, 0:00:12.790788s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 27/478 | 5.65%\n",
      "[0:05:57.426736<1:35:44.358450, 0:00:12.765241s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 28/478 | 5.86%\n",
      "[0:06:09.256100<1:35:17.103081, 0:00:12.732969s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 29/478 | 6.07%\n",
      "[0:06:20.221399<1:34:37.973056, 0:00:12.674047s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 30/478 | 6.28%\n",
      "[0:06:33.046607<1:34:27.478581, 0:00:12.678923s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 31/478 | 6.49%\n",
      "[0:06:46.030300<1:34:19.047362, 0:00:12.688447s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 32/478 | 6.69%\n",
      "[0:06:59.334838<1:34:14.666620, 0:00:12.707116s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 33/478 | 6.90%\n",
      "[0:07:11.795419<1:33:58.740060, 0:00:12.699865s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 34/478 | 7.11%\n",
      "[0:07:23.324474<1:33:31.221402, 0:00:12.666414s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 35/478 | 7.32%\n",
      "[0:07:39.736028<1:34:04.536690, 0:00:12.770445s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 36/478 | 7.53%\n",
      "[0:07:54.017281<1:34:09.773598, 0:00:12.811278s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 37/478 | 7.74%\n",
      "[0:08:09.073555<1:34:22.957080, 0:00:12.870357s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 38/478 | 7.95%\n",
      "[0:08:21.649758<1:34:06.775346, 0:00:12.862814s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 39/478 | 8.16%\n",
      "[0:08:34.754888<1:33:56.565936, 0:00:12.868872s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 40/478 | 8.37%\n",
      "[0:08:47.211141<1:33:39.299096, 0:00:12.858808s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 41/478 | 8.58%\n",
      "[0:08:57.338050<1:32:58.080668, 0:00:12.793763s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 42/478 | 8.79%\n",
      "[0:09:08.267162<1:32:26.423565, 0:00:12.750399s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 43/478 | 9.00%\n",
      "[0:09:21.813399<1:32:21.522924, 0:00:12.768486s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 44/478 | 9.21%\n",
      "[0:09:35.025903<1:32:13.026849, 0:00:12.778353s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 45/478 | 9.41%\n",
      "[0:09:45.346641<1:31:37.168464, 0:00:12.724927s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 46/478 | 9.62%\n",
      "[0:09:54.964972<1:30:55.955299, 0:00:12.658829s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 47/478 | 9.83%\n",
      "[0:10:05.896672<1:30:27.824210, 0:00:12.622847s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 48/478 | 10.04%\n",
      "[0:10:17.341455<1:30:04.887345, 0:00:12.598805s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 49/478 | 10.25%\n",
      "[0:10:30.549760<1:29:57.505860, 0:00:12.610995s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 50/478 | 10.46%\n",
      "[0:10:43.659599<1:29:49.071352, 0:00:12.620776s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 51/478 | 10.67%\n",
      "[0:10:53.840639<1:29:16.463508, 0:00:12.573858s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 52/478 | 10.88%\n",
      "[0:11:06.263236<1:29:02.676700, 0:00:12.571004s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 53/478 | 11.09%\n",
      "[0:11:17.167934<1:28:37.022328, 0:00:12.540147s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 54/478 | 11.30%\n",
      "[0:11:32.912577<1:28:49.127430, 0:00:12.598410s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 55/478 | 11.51%\n",
      "[0:11:45.014379<1:28:32.786724, 0:00:12.589542s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 56/478 | 11.72%\n",
      "[0:12:00.960050<1:28:44.985662, 0:00:12.648422s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 57/478 | 11.92%\n",
      "[0:12:14.837901<1:28:41.239980, 0:00:12.669619s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 58/478 | 12.13%\n",
      "[0:12:27.735428<1:28:30.188958, 0:00:12.673482s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 59/478 | 12.34%\n",
      "[0:12:42.591076<1:28:32.717718, 0:00:12.709851s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 60/478 | 12.55%\n",
      "[0:12:55.763764<1:28:23.172063, 0:00:12.717439s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 61/478 | 12.76%\n",
      "[0:13:09.108181<1:28:14.661216, 0:00:12.727551s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 62/478 | 12.97%\n",
      "[0:13:23.009850<1:28:09.668020, 0:00:12.746188s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 63/478 | 13.18%\n",
      "[0:13:36.372649<1:28:00.910722, 0:00:12.755823s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 64/478 | 13.39%\n",
      "[0:13:50.073165<1:27:54.157028, 0:00:12.770356s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 65/478 | 13.60%\n",
      "[0:14:03.927700<1:27:48.154596, 0:00:12.786783s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 66/478 | 13.81%\n",
      "[0:14:19.456020<1:27:52.185522, 0:00:12.827702s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 67/478 | 14.02%\n",
      "[0:14:36.387775<1:28:04.102960, 0:00:12.888056s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 68/478 | 14.23%\n",
      "[0:14:49.842567<1:27:54.574021, 0:00:12.896269s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 69/478 | 14.44%\n",
      "[0:15:04.142621<1:27:49.859784, 0:00:12.916323s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 70/478 | 14.64%\n",
      "[0:15:18.145026<1:27:43.169340, 0:00:12.931620s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 71/478 | 14.85%\n",
      "[0:15:31.100936<1:27:30.374542, 0:00:12.931957s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 72/478 | 15.06%\n",
      "[0:15:45.541383<1:27:25.811910, 0:00:12.952622s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 73/478 | 15.27%\n",
      "[0:15:58.002155<1:27:10.173900, 0:00:12.945975s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 74/478 | 15.48%\n",
      "[0:16:09.258599<1:26:48.149544, 0:00:12.923448s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 75/478 | 15.69%\n",
      "[0:16:20.991357<1:26:28.927962, 0:00:12.907781s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 76/478 | 15.90%\n",
      "[0:16:38.688137<1:26:40.960376, 0:00:12.969976s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 77/478 | 16.11%\n",
      "[0:16:49.961501<1:26:19.289600, 0:00:12.948224s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 78/478 | 16.32%\n",
      "[0:17:07.054944<1:26:27.277305, 0:00:13.000695s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 79/478 | 16.53%\n",
      "[0:17:19.915518<1:26:13.579712, 0:00:12.998944s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 80/478 | 16.74%\n",
      "[0:17:31.978597<1:25:55.993830, 0:00:12.987390s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 81/478 | 16.95%\n",
      "[0:17:46.565776<1:25:50.732400, 0:00:13.006900s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 82/478 | 17.15%\n",
      "[0:18:00.987440<1:25:44.458275, 0:00:13.023945s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 83/478 | 17.36%\n",
      "[0:18:13.516117<1:25:29.111306, 0:00:13.018049s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 84/478 | 17.57%\n",
      "[0:18:24.216447<1:25:05.377326, 0:00:12.990782s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 85/478 | 17.78%\n",
      "[0:18:35.912042<1:24:46.482632, 0:00:12.975721s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 86/478 | 17.99%\n",
      "[0:18:46.144024<1:24:21.175944, 0:00:12.944184s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 87/478 | 18.20%\n",
      "[0:19:00.329812<1:24:13.734270, 0:00:12.958293s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 88/478 | 18.41%\n",
      "[0:19:09.933798<1:23:46.114956, 0:00:12.920604s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 89/478 | 18.62%\n",
      "[0:19:19.478544<1:23:18.640860, 0:00:12.883095s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 90/478 | 18.83%\n",
      "[0:19:31.472188<1:23:01.975227, 0:00:12.873321s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 91/478 | 19.04%\n",
      "[0:19:41.624716<1:22:37.686342, 0:00:12.843747s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 92/478 | 19.25%\n",
      "[0:19:53.482701<1:22:20.761595, 0:00:12.833147s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 93/478 | 19.46%\n",
      "[0:20:06.229543<1:22:07.575936, 0:00:12.832229s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 94/478 | 19.67%\n",
      "[0:20:18.479629<1:21:52.396683, 0:00:12.826101s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 95/478 | 19.87%\n",
      "[0:20:31.073403<1:21:38.646142, 0:00:12.823681s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 96/478 | 20.08%\n",
      "[0:20:42.269437<1:21:19.429281, 0:00:12.806901s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 97/478 | 20.29%\n",
      "[0:20:55.474336<1:21:08.165940, 0:00:12.810963s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 98/478 | 20.50%\n",
      "[0:21:09.593787<1:21:00.364220, 0:00:12.824180s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 99/478 | 20.71%\n",
      "[0:21:21.562457<1:20:44.306250, 0:00:12.815625s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 100/478 | 20.92%\n",
      "[0:21:33.741522<1:20:29.114394, 0:00:12.809322s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 101/478 | 21.13%\n",
      "[0:21:44.304074<1:20:08.022920, 0:00:12.787295s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 102/478 | 21.34%\n",
      "[0:21:57.015950<1:19:54.961125, 0:00:12.786563s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 103/478 | 21.55%\n",
      "[0:22:09.004040<1:19:39.302990, 0:00:12.778885s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 104/478 | 21.76%\n",
      "[0:22:22.054972<1:19:27.490548, 0:00:12.781476s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 105/478 | 21.97%\n",
      "[0:22:37.746040<1:19:24.920100, 0:00:12.808925s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 106/478 | 22.18%\n",
      "[0:22:49.616073<1:19:08.855650, 0:00:12.800150s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 107/478 | 22.38%\n",
      "[0:23:01.946047<1:18:54.444890, 0:00:12.795797s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 108/478 | 22.59%\n",
      "[0:23:13.473016<1:18:37.353564, 0:00:12.784156s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 109/478 | 22.80%\n",
      "[0:23:26.839268<1:18:26.516864, 0:00:12.789448s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 110/478 | 23.01%\n",
      "[0:23:37.902123<1:18:08.018731, 0:00:12.773893s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 111/478 | 23.22%\n",
      "[0:23:52.833052<1:18:02.293632, 0:00:12.793152s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 112/478 | 23.43%\n",
      "[0:24:04.928907<1:17:47.248065, 0:00:12.786981s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 113/478 | 23.64%\n",
      "[0:24:18.309212<1:17:36.355704, 0:00:12.792186s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 114/478 | 23.85%\n",
      "[0:24:32.524592<1:17:28.056006, 0:00:12.804562s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 115/478 | 24.06%\n",
      "[0:24:44.954589<1:17:14.082546, 0:00:12.801333s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 116/478 | 24.27%\n",
      "[0:24:58.278854<1:17:02.894522, 0:00:12.805802s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 117/478 | 24.48%\n",
      "[0:25:11.301045<1:16:50.748960, 0:00:12.807636s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 118/478 | 24.69%\n",
      "[0:25:28.165191<1:16:50.178916, 0:00:12.841724s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 119/478 | 24.90%\n",
      "[0:25:41.178599<1:16:37.849490, 0:00:12.843155s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 120/478 | 25.10%\n",
      "[0:25:53.355224<1:16:23.039622, 0:00:12.837646s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 121/478 | 25.31%\n",
      "[0:26:04.969343<1:16:06.632008, 0:00:12.827618s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 122/478 | 25.52%\n",
      "[0:26:19.333482<1:15:58.239050, 0:00:12.840110s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 123/478 | 25.73%\n",
      "[0:26:30.235194<1:15:39.864858, 0:00:12.824477s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 124/478 | 25.94%\n",
      "[0:26:43.776160<1:15:29.063777, 0:00:12.830209s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 125/478 | 26.15%\n",
      "[0:26:56.076724<1:15:14.754112, 0:00:12.826006s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 126/478 | 26.36%\n",
      "[0:27:10.655738<1:15:06.772959, 0:00:12.839809s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 127/478 | 26.57%\n",
      "[0:27:23.313753<1:14:53.436150, 0:00:12.838389s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 128/478 | 26.78%\n",
      "[0:27:37.066762<1:14:43.072171, 0:00:12.845479s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 129/478 | 26.99%\n",
      "[0:27:51.548945<1:14:34.608012, 0:00:12.858069s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 130/478 | 27.20%\n",
      "[0:28:04.259565<1:14:21.359221, 0:00:12.856943s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 131/478 | 27.41%\n",
      "[0:28:16.614335<1:14:07.186094, 0:00:12.853139s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 132/478 | 27.62%\n",
      "[0:28:31.922716<1:14:00.701655, 0:00:12.871599s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 133/478 | 27.82%\n",
      "[0:28:42.795309<1:13:42.698264, 0:00:12.856681s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 134/478 | 28.03%\n",
      "[0:28:54.484890<1:13:26.876348, 0:00:12.848036s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 135/478 | 28.24%\n",
      "[0:29:10.152850<1:13:21.119682, 0:00:12.868771s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 136/478 | 28.45%\n",
      "[0:29:25.544063<1:13:14.529403, 0:00:12.887183s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 137/478 | 28.66%\n",
      "[0:29:36.548011<1:12:57.002240, 0:00:12.873536s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 138/478 | 28.87%\n",
      "[0:29:47.598967<1:12:39.683736, 0:00:12.860424s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 139/478 | 29.08%\n",
      "[0:30:00.758545<1:12:27.545618, 0:00:12.862561s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 140/478 | 29.29%\n",
      "[0:30:13.648414<1:12:14.748435, 0:00:12.862755s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 141/478 | 29.50%\n",
      "[0:30:27.279141<1:12:03.702768, 0:00:12.868163s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 142/478 | 29.71%\n",
      "[0:30:39.207303<1:11:48.632650, 0:00:12.861590s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 143/478 | 29.92%\n",
      "[0:30:48.759214<1:11:28.094404, 0:00:12.838606s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 144/478 | 30.13%\n",
      "[0:30:59.197522<1:11:09.743316, 0:00:12.822052s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 145/478 | 30.33%\n",
      "[0:31:10.750499<1:10:54.035520, 0:00:12.813360s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 146/478 | 30.54%\n",
      "[0:31:21.580701<1:10:36.756639, 0:00:12.799869s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 147/478 | 30.75%\n",
      "[0:31:36.433504<1:10:28.534200, 0:00:12.813740s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 148/478 | 30.96%\n",
      "[0:31:49.383986<1:10:16.022482, 0:00:12.814658s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 149/478 | 31.17%\n",
      "[0:32:03.970063<1:10:07.081176, 0:00:12.826467s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 150/478 | 31.38%\n",
      "[0:32:19.796840<1:10:00.752199, 0:00:12.846337s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 151/478 | 31.59%\n",
      "[0:32:33.311674<1:09:49.339610, 0:00:12.850735s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 152/478 | 31.80%\n",
      "[0:32:46.791371<1:09:37.824950, 0:00:12.854846s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 153/478 | 32.01%\n",
      "[0:32:59.245215<1:09:24.126408, 0:00:12.852242s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 154/478 | 32.22%\n",
      "[0:33:11.105431<1:09:09.206643, 0:00:12.845841s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 155/478 | 32.43%\n",
      "[0:33:23.199858<1:08:54.810050, 0:00:12.841025s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 156/478 | 32.64%\n",
      "[0:33:36.342951<1:08:42.586629, 0:00:12.842949s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 157/478 | 32.85%\n",
      "[0:33:50.358827<1:08:32.119040, 0:00:12.850372s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 158/478 | 33.05%\n",
      "[0:34:02.950982<1:08:18.750612, 0:00:12.848748s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 159/478 | 33.26%\n",
      "[0:34:16.657359<1:08:07.606344, 0:00:12.854108s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 160/478 | 33.47%\n",
      "[0:34:31.420072<1:07:58.510271, 0:00:12.865963s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 161/478 | 33.68%\n",
      "[0:34:42.016442<1:07:41.217148, 0:00:12.851953s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 162/478 | 33.89%\n",
      "[0:34:56.292377<1:07:31.117035, 0:00:12.860689s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 163/478 | 34.10%\n",
      "[0:35:11.132150<1:07:22.045698, 0:00:12.872757s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 164/478 | 34.31%\n",
      "[0:35:27.366023<1:07:15.548751, 0:00:12.893127s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 165/478 | 34.52%\n",
      "[0:35:38.266489<1:06:58.910376, 0:00:12.881123s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 166/478 | 34.73%\n",
      "[0:35:52.123067<1:06:47.845804, 0:00:12.886964s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 167/478 | 34.94%\n",
      "[0:36:04.001473<1:06:33.097910, 0:00:12.880961s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 168/478 | 35.15%\n",
      "[0:36:16.655751<1:06:19.802580, 0:00:12.879620s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 169/478 | 35.36%\n",
      "[0:36:27.455483<1:06:03.154580, 0:00:12.867385s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 170/478 | 35.56%\n",
      "[0:36:39.737913<1:05:49.236948, 0:00:12.863964s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 171/478 | 35.77%\n",
      "[0:36:57.112789<1:05:44.398446, 0:00:12.890191s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 172/478 | 35.98%\n",
      "[0:37:12.200994<1:05:35.383280, 0:00:12.902896s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 173/478 | 36.19%\n",
      "[0:37:23.753619<1:05:20.121344, 0:00:12.895136s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 174/478 | 36.40%\n",
      "[0:37:39.596338<1:05:12.329637, 0:00:12.911979s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 175/478 | 36.61%\n",
      "[0:37:54.741479<1:05:03.249434, 0:00:12.924667s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 176/478 | 36.82%\n",
      "[0:38:06.577848<1:04:48.474219, 0:00:12.918519s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 177/478 | 37.03%\n",
      "[0:38:21.166027<1:04:38.369700, 0:00:12.927899s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 178/478 | 37.24%\n",
      "[0:38:36.992685<1:04:30.283807, 0:00:12.944093s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 179/478 | 37.45%\n",
      "[0:38:50.235164<1:04:17.833798, 0:00:12.945751s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 180/478 | 37.66%\n",
      "[0:39:01.459327<1:04:02.062983, 0:00:12.936239s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 181/478 | 37.87%\n",
      "[0:39:15.582000<1:03:51.056368, 0:00:12.942758s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 182/478 | 38.08%\n",
      "[0:39:29.153941<1:03:39.127820, 0:00:12.946196s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 183/478 | 38.28%\n",
      "[0:39:41.633179<1:03:25.435746, 0:00:12.943659s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 184/478 | 38.49%\n",
      "[0:39:53.293886<1:03:10.460132, 0:00:12.936724s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 185/478 | 38.70%\n",
      "[0:40:05.570261<1:02:56.486516, 0:00:12.933173s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 186/478 | 38.91%\n",
      "[0:40:17.406445<1:02:41.846337, 0:00:12.927307s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 187/478 | 39.12%\n",
      "[0:40:30.482476<1:02:29.148420, 0:00:12.928098s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 188/478 | 39.33%\n",
      "[0:40:45.376415<1:02:19.226500, 0:00:12.938500s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 189/478 | 39.54%\n",
      "[0:40:58.701968<1:02:06.874656, 0:00:12.940537s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 190/478 | 39.75%\n",
      "[0:41:10.247863<1:01:51.838445, 0:00:12.933235s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 191/478 | 39.96%\n",
      "[0:41:22.868363<1:01:38.439316, 0:00:12.931606s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 192/478 | 40.17%\n",
      "[0:41:35.770225<1:01:25.463820, 0:00:12.931452s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 193/478 | 40.38%\n",
      "[0:41:48.529668<1:01:12.280460, 0:00:12.930565s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 194/478 | 40.59%\n",
      "[0:42:01.186645<1:00:58.952846, 0:00:12.929162s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 195/478 | 40.79%\n",
      "[0:42:16.182329<1:00:48.997092, 0:00:12.939706s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 196/478 | 41.00%\n",
      "[0:42:31.471046<1:00:39.408030, 0:00:12.951630s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 197/478 | 41.21%\n",
      "[0:42:45.255948<1:00:27.634640, 0:00:12.955838s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 198/478 | 41.42%\n",
      "[0:42:56.196467<1:00:11.853369, 0:00:12.945711s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 199/478 | 41.63%\n",
      "[0:43:07.648103<0:59:56.830998, 0:00:12.938241s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 200/478 | 41.84%\n",
      "[0:43:21.163585<0:59:44.688024, 0:00:12.941112s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 201/478 | 42.05%\n",
      "[0:43:32.815283<0:59:29.985204, 0:00:12.934729s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 202/478 | 42.26%\n",
      "[0:43:46.395913<0:59:17.925525, 0:00:12.937911s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 203/478 | 42.47%\n",
      "[0:43:57.635989<0:59:02.707112, 0:00:12.929588s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 204/478 | 42.68%\n",
      "[0:44:10.759900<0:58:50.036328, 0:00:12.930536s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 205/478 | 42.89%\n",
      "[0:44:21.732493<0:58:34.520704, 0:00:12.921032s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 206/478 | 43.10%\n",
      "[0:44:32.793712<0:58:19.164737, 0:00:12.912047s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 207/478 | 43.31%\n",
      "[0:44:46.185702<0:58:06.875580, 0:00:12.914354s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 208/478 | 43.51%\n",
      "[0:44:59.555775<0:57:54.547915, 0:00:12.916535s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 209/478 | 43.72%\n",
      "[0:45:10.180042<0:57:38.705892, 0:00:12.905619s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 210/478 | 43.93%\n",
      "[0:45:23.194161<0:57:25.937511, 0:00:12.906133s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 211/478 | 44.14%\n",
      "[0:45:35.683951<0:57:12.509220, 0:00:12.904170s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 212/478 | 44.35%\n",
      "[0:45:47.861116<0:56:58.700340, 0:00:12.900756s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 213/478 | 44.56%\n",
      "[0:46:00.428126<0:56:45.388008, 0:00:12.899197s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 214/478 | 44.77%\n",
      "[0:46:12.655129<0:56:31.666410, 0:00:12.896070s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 215/478 | 44.98%\n",
      "[0:46:23.046496<0:56:15.732450, 0:00:12.884475s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 216/478 | 45.19%\n",
      "[0:46:37.193481<0:56:04.366473, 0:00:12.890293s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 217/478 | 45.40%\n",
      "[0:46:48.425571<0:55:49.498360, 0:00:12.882686s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 218/478 | 45.61%\n",
      "[0:46:59.545311<0:55:34.530724, 0:00:12.874636s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 219/478 | 45.82%\n",
      "[0:47:11.862563<0:55:21.002574, 0:00:12.872103s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 220/478 | 46.03%\n",
      "[0:47:25.796219<0:55:09.364842, 0:00:12.876906s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 221/478 | 46.23%\n",
      "[0:47:38.864079<0:54:56.708096, 0:00:12.877766s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 222/478 | 46.44%\n",
      "[0:47:52.625940<0:54:44.841405, 0:00:12.881731s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 223/478 | 46.65%\n",
      "[0:48:03.441415<0:54:29.616524, 0:00:12.872506s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 224/478 | 46.86%\n",
      "[0:48:15.479468<0:54:15.805894, 0:00:12.868798s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 225/478 | 47.07%\n",
      "[0:48:27.825438<0:54:02.353968, 0:00:12.866484s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 226/478 | 47.28%\n",
      "[0:48:43.879459<0:53:53.012026, 0:00:12.880526s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 227/478 | 47.49%\n",
      "[0:48:56.214094<0:53:39.533000, 0:00:12.878132s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 228/478 | 47.70%\n",
      "[0:49:08.107694<0:53:25.584417, 0:00:12.873833s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 229/478 | 47.91%\n",
      "[0:49:21.834469<0:53:13.630168, 0:00:12.877541s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 230/478 | 48.12%\n",
      "[0:49:35.670041<0:53:01.776936, 0:00:12.881688s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 231/478 | 48.33%\n",
      "[0:49:46.513481<0:52:46.734138, 0:00:12.872903s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 232/478 | 48.54%\n",
      "[0:49:57.861708<0:52:32.257955, 0:00:12.866359s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 233/478 | 48.74%\n",
      "[0:50:12.497502<0:52:21.236724, 0:00:12.873921s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 234/478 | 48.95%\n",
      "[0:50:24.167918<0:52:07.118400, 0:00:12.868800s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 235/478 | 49.16%\n",
      "[0:50:36.388297<0:51:53.584584, 0:00:12.866052s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 236/478 | 49.37%\n",
      "[0:50:50.384722<0:51:41.868102, 0:00:12.870822s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 237/478 | 49.58%\n",
      "[0:50:59.959761<0:51:25.673760, 0:00:12.856974s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 238/478 | 49.79%\n",
      "[0:51:10.818758<0:51:10.818746, 0:00:12.848614s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 239/478 | 50.00%\n",
      "[0:51:23.328214<0:50:57.633838, 0:00:12.847201s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 240/478 | 50.21%\n",
      "[0:51:33.317335<0:50:41.976054, 0:00:12.835342s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 241/478 | 50.42%\n",
      "[0:51:47.149824<0:50:30.113032, 0:00:12.839462s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 242/478 | 50.63%\n",
      "[0:51:58.477692<0:50:15.811870, 0:00:12.833242s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 243/478 | 50.84%\n",
      "[0:52:11.942387<0:50:03.583986, 0:00:12.835829s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 244/478 | 51.05%\n",
      "[0:52:24.764891<0:49:50.735575, 0:00:12.835775s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 245/478 | 51.26%\n",
      "[0:52:39.278363<0:49:39.482040, 0:00:12.842595s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 246/478 | 51.46%\n",
      "[0:52:51.361920<0:49:25.929582, 0:00:12.839522s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 247/478 | 51.67%\n",
      "[0:53:04.177509<0:49:13.067750, 0:00:12.839425s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 248/478 | 51.88%\n",
      "[0:53:16.108011<0:48:59.392475, 0:00:12.835775s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 249/478 | 52.09%\n",
      "[0:53:30.399745<0:48:47.884572, 0:00:12.841599s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 250/478 | 52.30%\n",
      "[0:53:44.919060<0:48:36.560241, 0:00:12.848283s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 251/478 | 52.51%\n",
      "[0:53:56.033066<0:48:22.156626, 0:00:12.841401s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 252/478 | 52.72%\n",
      "[0:54:06.804308<0:48:07.474275, 0:00:12.833219s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 253/478 | 52.93%\n",
      "[0:54:16.939520<0:47:52.261728, 0:00:12.822597s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 254/478 | 53.14%\n",
      "[0:54:33.236820<0:47:42.477729, 0:00:12.836223s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 255/478 | 53.35%\n",
      "[0:54:45.193058<0:47:28.878270, 0:00:12.832785s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 256/478 | 53.56%\n",
      "[0:54:57.654475<0:47:15.726140, 0:00:12.831340s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 257/478 | 53.77%\n",
      "[0:55:09.338556<0:47:01.916680, 0:00:12.826894s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 258/478 | 53.97%\n",
      "[0:55:24.042426<0:46:50.676879, 0:00:12.834141s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 259/478 | 54.18%\n",
      "[0:55:37.238528<0:46:38.146194, 0:00:12.835533s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 260/478 | 54.39%\n",
      "[0:55:50.233683<0:46:25.443248, 0:00:12.836144s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 261/478 | 54.60%\n",
      "[0:56:03.742442<0:46:13.161792, 0:00:12.838712s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 262/478 | 54.81%\n",
      "[0:56:16.451859<0:46:00.217300, 0:00:12.838220s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 263/478 | 55.02%\n",
      "[0:56:30.047434<0:45:47.993046, 0:00:12.841089s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 264/478 | 55.23%\n",
      "[0:56:44.116338<0:45:36.138786, 0:00:12.845722s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 265/478 | 55.44%\n",
      "[0:56:56.383841<0:45:22.832176, 0:00:12.843548s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 266/478 | 55.65%\n",
      "[0:57:09.470481<0:45:10.180849, 0:00:12.844459s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 267/478 | 55.86%\n",
      "[0:57:23.372990<0:44:58.165470, 0:00:12.848407s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 268/478 | 56.07%\n",
      "[0:57:38.982147<0:44:47.462030, 0:00:12.858670s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 269/478 | 56.28%\n",
      "[0:57:52.293562<0:44:34.952176, 0:00:12.860347s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 270/478 | 56.49%\n",
      "[0:58:07.116206<0:44:23.590509, 0:00:12.867587s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 271/478 | 56.69%\n",
      "[0:58:21.462356<0:44:11.842738, 0:00:12.873023s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 272/478 | 56.90%\n",
      "[0:58:34.573332<0:43:59.148475, 0:00:12.873895s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 273/478 | 57.11%\n",
      "[0:58:48.011471<0:43:46.694616, 0:00:12.875954s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 274/478 | 57.32%\n",
      "[0:59:00.570821<0:43:33.585009, 0:00:12.874803s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 275/478 | 57.53%\n",
      "[0:59:10.814950<0:43:18.784944, 0:00:12.865272s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 276/478 | 57.74%\n",
      "[0:59:22.196189<0:43:04.842714, 0:00:12.859914s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 277/478 | 57.95%\n",
      "[0:59:32.315827<0:42:50.011400, 0:00:12.850057s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 278/478 | 58.16%\n",
      "[0:59:43.435283<0:42:35.926946, 0:00:12.843854s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 279/478 | 58.37%\n",
      "[0:59:53.535018<0:42:21.142692, 0:00:12.834054s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 280/478 | 58.58%\n",
      "[1:00:06.814604<0:42:08.620883, 0:00:12.835639s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 281/478 | 58.79%\n",
      "[1:00:20.083599<0:41:56.086496, 0:00:12.837176s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 282/478 | 59.00%\n",
      "[1:00:32.737552<0:41:43.122960, 0:00:12.836528s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 283/478 | 59.21%\n",
      "[1:00:45.015193<0:41:29.904834, 0:00:12.834561s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 284/478 | 59.41%\n",
      "[1:00:57.458360<0:41:16.805091, 0:00:12.833187s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 285/478 | 59.62%\n",
      "[1:01:09.731663<0:41:03.596160, 0:00:12.831230s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 286/478 | 59.83%\n",
      "[1:01:24.574549<0:40:52.103649, 0:00:12.838239s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 287/478 | 60.04%\n",
      "[1:01:36.074894<0:40:38.382670, 0:00:12.833593s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 288/478 | 60.25%\n",
      "[1:01:45.631198<0:40:23.405817, 0:00:12.822253s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 289/478 | 60.46%\n",
      "[1:01:58.322781<0:40:10.498964, 0:00:12.821803s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 290/478 | 60.67%\n",
      "[1:02:11.792022<0:39:58.093236, 0:00:12.824028s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 291/478 | 60.88%\n",
      "[1:02:25.245274<0:39:45.669852, 0:00:12.826182s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 292/478 | 61.09%\n",
      "[1:02:39.182143<0:39:33.545005, 0:00:12.829973s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 293/478 | 61.30%\n",
      "[1:02:52.824660<0:39:21.223608, 0:00:12.832737s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 294/478 | 61.51%\n",
      "[1:03:07.624895<0:39:09.611298, 0:00:12.839406s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 295/478 | 61.72%\n",
      "[1:03:21.522118<0:38:57.422360, 0:00:12.842980s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 296/478 | 61.92%\n",
      "[1:03:33.155396<0:38:43.842167, 0:00:12.838907s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 297/478 | 62.13%\n",
      "[1:03:44.257505<0:38:29.954220, 0:00:12.833079s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 298/478 | 62.34%\n",
      "[1:03:57.834638<0:38:17.566493, 0:00:12.835567s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 299/478 | 62.55%\n",
      "[1:04:11.544997<0:38:05.249974, 0:00:12.838483s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 300/478 | 62.76%\n",
      "[1:04:24.100066<0:37:52.244934, 0:00:12.837542s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 301/478 | 62.97%\n",
      "[1:04:34.120008<0:37:37.765312, 0:00:12.828212s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 302/478 | 63.18%\n",
      "[1:04:45.274247<0:37:23.970225, 0:00:12.822687s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 303/478 | 63.39%\n",
      "[1:04:57.970891<0:37:11.075502, 0:00:12.822273s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 304/478 | 63.60%\n",
      "[1:05:10.483669<0:36:58.077634, 0:00:12.821258s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 305/478 | 63.81%\n",
      "[1:05:22.978880<0:36:45.073024, 0:00:12.820192s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 306/478 | 64.02%\n",
      "[1:05:33.028150<0:36:30.709557, 0:00:12.811167s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 307/478 | 64.23%\n",
      "[1:05:45.439500<0:36:17.677730, 0:00:12.809869s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 308/478 | 64.44%\n",
      "[1:05:58.189629<0:36:04.835075, 0:00:12.809675s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 309/478 | 64.64%\n",
      "[1:06:13.986583<0:35:53.644416, 0:00:12.819312s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 310/478 | 64.85%\n",
      "[1:06:26.721217<0:35:40.779513, 0:00:12.819039s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 311/478 | 65.06%\n",
      "[1:06:39.604192<0:35:27.994504, 0:00:12.819244s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 312/478 | 65.27%\n",
      "[1:06:52.675940<0:35:15.308415, 0:00:12.820051s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 313/478 | 65.48%\n",
      "[1:07:04.150044<0:35:01.785296, 0:00:12.815764s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 314/478 | 65.69%\n",
      "[1:07:18.287151<0:34:49.653317, 0:00:12.819959s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 315/478 | 65.90%\n",
      "[1:07:29.266480<0:34:35.889708, 0:00:12.814134s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 316/478 | 66.11%\n",
      "[1:07:40.123684<0:34:22.081721, 0:00:12.807961s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 317/478 | 66.32%\n",
      "[1:07:56.221481<0:34:10.929120, 0:00:12.818307s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 318/478 | 66.53%\n",
      "[1:08:07.188217<0:33:57.187818, 0:00:12.812502s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 319/478 | 66.74%\n",
      "[1:08:21.576518<0:33:45.153466, 0:00:12.817427s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 320/478 | 66.95%\n",
      "[1:08:33.072555<0:33:31.689670, 0:00:12.813310s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 321/478 | 67.15%\n",
      "[1:08:48.624576<0:33:20.203140, 0:00:12.821815s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 322/478 | 67.36%\n",
      "[1:09:03.852281<0:33:08.535920, 0:00:12.829264s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 323/478 | 67.57%\n",
      "[1:09:16.124651<0:32:55.441930, 0:00:12.827545s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 324/478 | 67.78%\n",
      "[1:09:28.650379<0:32:42.472401, 0:00:12.826617s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 325/478 | 67.99%\n",
      "[1:09:39.015381<0:32:28.498032, 0:00:12.819066s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 326/478 | 68.20%\n",
      "[1:09:51.414048<0:32:15.484780, 0:00:12.817780s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 327/478 | 68.41%\n",
      "[1:10:02.484801<0:32:01.868100, 0:00:12.812454s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 328/478 | 68.62%\n",
      "[1:10:16.707289<0:31:49.694111, 0:00:12.816739s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 329/478 | 68.83%\n",
      "[1:10:31.265166<0:31:37.658368, 0:00:12.822016s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 330/478 | 69.04%\n",
      "[1:10:42.190300<0:31:23.993895, 0:00:12.816285s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 331/478 | 69.25%\n",
      "[1:10:54.530653<0:31:10.968246, 0:00:12.814851s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 332/478 | 69.46%\n",
      "[1:11:06.508163<0:30:57.788865, 0:00:12.812337s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 333/478 | 69.67%\n",
      "[1:11:20.083012<0:30:45.305280, 0:00:12.814620s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 334/478 | 69.87%\n",
      "[1:11:30.645943<0:30:31.529414, 0:00:12.807898s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 335/478 | 70.08%\n",
      "[1:11:42.160101<0:30:18.174816, 0:00:12.804048s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 336/478 | 70.29%\n",
      "[1:11:54.422987<0:30:05.144322, 0:00:12.802442s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 337/478 | 70.50%\n",
      "[1:12:06.373993<0:29:51.989220, 0:00:12.799923s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 338/478 | 70.71%\n",
      "[1:12:18.857290<0:29:39.059471, 0:00:12.798989s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 339/478 | 70.92%\n",
      "[1:12:32.457459<0:29:26.585610, 0:00:12.801345s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 340/478 | 71.13%\n",
      "[1:12:46.292116<0:29:14.199512, 0:00:12.804376s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 341/478 | 71.34%\n",
      "[1:12:56.949827<0:29:00.541464, 0:00:12.798099s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 342/478 | 71.55%\n",
      "[1:13:09.827768<0:28:47.774820, 0:00:12.798332s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 343/478 | 71.76%\n",
      "[1:13:23.783068<0:28:35.427130, 0:00:12.801695s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 344/478 | 71.97%\n",
      "[1:13:37.133540<0:28:22.837038, 0:00:12.803286s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 345/478 | 72.18%\n",
      "[1:13:48.277027<0:28:09.400548, 0:00:12.798489s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 346/478 | 72.38%\n",
      "[1:14:02.708993<0:27:57.218676, 0:00:12.803196s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 347/478 | 72.59%\n",
      "[1:14:14.063363<0:27:43.874290, 0:00:12.799033s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 348/478 | 72.80%\n",
      "[1:14:24.698318<0:27:30.275328, 0:00:12.792832s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 349/478 | 73.01%\n",
      "[1:14:36.930995<0:27:17.277568, 0:00:12.791231s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 350/478 | 73.22%\n",
      "[1:14:50.708921<0:27:04.843461, 0:00:12.794043s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 351/478 | 73.43%\n",
      "[1:15:03.038407<0:26:51.883098, 0:00:12.792723s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 352/478 | 73.64%\n",
      "[1:15:15.261243<0:26:38.888500, 0:00:12.791108s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 353/478 | 73.85%\n",
      "[1:15:24.879299<0:26:24.985980, 0:00:12.782145s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 354/478 | 74.06%\n",
      "[1:15:37.224819<0:26:12.052545, 0:00:12.780915s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 355/478 | 74.27%\n",
      "[1:15:49.014935<0:25:58.932104, 0:00:12.778132s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 356/478 | 74.48%\n",
      "[1:16:02.975050<0:25:46.554603, 0:00:12.781443s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 357/478 | 74.69%\n",
      "[1:16:18.321237<0:25:34.632840, 0:00:12.788607s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 358/478 | 74.90%\n",
      "[1:16:28.371671<0:25:20.936620, 0:00:12.780980s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 359/478 | 75.10%\n",
      "[1:16:43.275343<0:25:08.851368, 0:00:12.786876s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 360/478 | 75.31%\n",
      "[1:16:54.787884<0:24:55.651482, 0:00:12.783346s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 361/478 | 75.52%\n",
      "[1:17:08.041324<0:24:43.018820, 0:00:12.784645s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 362/478 | 75.73%\n",
      "[1:17:20.062405<0:24:29.992215, 0:00:12.782541s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 363/478 | 75.94%\n",
      "[1:17:33.322699<0:24:17.359356, 0:00:12.783854s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 364/478 | 76.15%\n",
      "[1:17:43.672965<0:24:03.822018, 0:00:12.777186s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 365/478 | 76.36%\n",
      "[1:17:56.269562<0:23:50.989616, 0:00:12.776693s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 366/478 | 76.57%\n",
      "[1:18:06.273545<0:23:37.374318, 0:00:12.769138s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 367/478 | 76.78%\n",
      "[1:18:18.040599<0:23:24.305650, 0:00:12.766415s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 368/478 | 76.99%\n",
      "[1:18:28.883755<0:23:10.971127, 0:00:12.761203s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 369/478 | 77.20%\n",
      "[1:18:42.708880<0:22:58.520424, 0:00:12.764078s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 370/478 | 77.41%\n",
      "[1:18:53.576164<0:22:45.209255, 0:00:12.758965s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 371/478 | 77.62%\n",
      "[1:19:04.366157<0:22:31.889232, 0:00:12.753672s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 372/478 | 77.82%\n",
      "[1:19:17.025182<0:22:19.108995, 0:00:12.753419s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 373/478 | 78.03%\n",
      "[1:19:29.618659<0:22:06.311064, 0:00:12.752991s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 374/478 | 78.24%\n",
      "[1:19:43.126003<0:21:53.765309, 0:00:12.755003s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 375/478 | 78.45%\n",
      "[1:19:56.836689<0:21:41.269488, 0:00:12.757544s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 376/478 | 78.66%\n",
      "[1:20:08.789271<0:21:28.296309, 0:00:12.755409s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 377/478 | 78.87%\n",
      "[1:20:20.833303<0:21:15.352700, 0:00:12.753527s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 378/478 | 79.08%\n",
      "[1:20:33.111158<0:21:02.474928, 0:00:12.752272s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 379/478 | 79.29%\n",
      "[1:20:43.943869<0:20:49.227658, 0:00:12.747221s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 380/478 | 79.50%\n",
      "[1:20:56.537595<0:20:36.441346, 0:00:12.746818s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 381/478 | 79.71%\n",
      "[1:21:10.415953<0:20:23.978880, 0:00:12.749780s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 382/478 | 79.92%\n",
      "[1:21:20.779167<0:20:10.637155, 0:00:12.743549s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 383/478 | 80.13%\n",
      "[1:21:35.245325<0:19:58.315290, 0:00:12.748035s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 384/478 | 80.33%\n",
      "[1:21:45.506867<0:19:44.966568, 0:00:12.741576s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 385/478 | 80.54%\n",
      "[1:22:00.588123<0:19:32.782696, 0:00:12.747638s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 386/478 | 80.75%\n",
      "[1:22:14.687300<0:19:20.352830, 0:00:12.751130s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 387/478 | 80.96%\n",
      "[1:22:27.524943<0:19:07.621770, 0:00:12.751353s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 388/478 | 81.17%\n",
      "[1:22:38.762407<0:18:54.524029, 0:00:12.747461s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 389/478 | 81.38%\n",
      "[1:22:49.463605<0:18:41.314832, 0:00:12.742214s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 390/478 | 81.59%\n",
      "[1:23:01.833937<0:18:28.489881, 0:00:12.741263s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 391/478 | 81.80%\n",
      "[1:23:14.635039<0:18:15.761776, 0:00:12.741416s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 392/478 | 82.01%\n",
      "[1:23:28.539588<0:18:03.271960, 0:00:12.744376s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 393/478 | 82.22%\n",
      "[1:23:46.765948<0:17:51.696276, 0:00:12.758289s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 394/478 | 82.43%\n",
      "[1:24:02.115354<0:17:39.482467, 0:00:12.764849s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 395/478 | 82.64%\n",
      "[1:24:15.603008<0:17:26.867268, 0:00:12.766674s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 396/478 | 82.85%\n",
      "[1:24:26.486849<0:17:13.716492, 0:00:12.761932s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 397/478 | 83.05%\n",
      "[1:24:40.451738<0:17:01.196320, 0:00:12.764954s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 398/478 | 83.26%\n",
      "[1:24:54.716724<0:16:48.728406, 0:00:12.768714s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 399/478 | 83.47%\n",
      "[1:25:08.283577<0:16:36.115302, 0:00:12.770709s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 400/478 | 83.68%\n",
      "[1:25:20.481000<0:16:23.234483, 0:00:12.769279s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 401/478 | 83.89%\n",
      "[1:25:32.880298<0:16:10.395284, 0:00:12.768359s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 402/478 | 84.10%\n",
      "[1:25:48.346594<0:15:58.129050, 0:00:12.775054s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 403/478 | 84.31%\n",
      "[1:26:00.024872<0:15:45.153086, 0:00:12.772339s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 404/478 | 84.52%\n",
      "[1:26:10.943423<0:15:32.046626, 0:00:12.767762s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 405/478 | 84.73%\n",
      "[1:26:25.420762<0:15:19.581984, 0:00:12.771972s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 406/478 | 84.94%\n",
      "[1:26:37.881008<0:15:06.755626, 0:00:12.771206s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 407/478 | 85.15%\n",
      "[1:26:49.214784<0:14:53.737810, 0:00:12.767683s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 408/478 | 85.36%\n",
      "[1:27:02.780661<0:14:41.104815, 0:00:12.769635s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 409/478 | 85.56%\n",
      "[1:27:18.461532<0:14:28.817980, 0:00:12.776735s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 410/478 | 85.77%\n",
      "[1:27:29.033262<0:14:15.681790, 0:00:12.771370s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 411/478 | 85.98%\n",
      "[1:27:43.973122<0:14:03.257844, 0:00:12.776634s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 412/478 | 86.19%\n",
      "[1:27:57.024679<0:13:50.524435, 0:00:12.777299s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 413/478 | 86.40%\n",
      "[1:28:08.581751<0:13:37.558528, 0:00:12.774352s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 414/478 | 86.61%\n",
      "[1:28:20.856006<0:13:24.708261, 0:00:12.773147s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 415/478 | 86.82%\n",
      "[1:28:36.762256<0:13:12.402036, 0:00:12.780678s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 416/478 | 87.03%\n",
      "[1:28:49.181806<0:12:59.568532, 0:00:12.779812s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 417/478 | 87.24%\n",
      "[1:29:02.163943<0:12:46.817820, 0:00:12.780297s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 418/478 | 87.45%\n",
      "[1:29:14.497594<0:12:33.974629, 0:00:12.779231s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 419/478 | 87.66%\n",
      "[1:29:27.219599<0:12:21.187452, 0:00:12.779094s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 420/478 | 87.87%\n",
      "[1:29:37.593745<0:12:08.082774, 0:00:12.773382s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 421/478 | 88.08%\n",
      "[1:29:51.520779<0:11:55.462496, 0:00:12.776116s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 422/478 | 88.28%\n",
      "[1:30:02.533651<0:11:42.457085, 0:00:12.771947s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 423/478 | 88.49%\n",
      "[1:30:13.118692<0:11:29.406606, 0:00:12.766789s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 424/478 | 88.70%\n",
      "[1:30:26.269123<0:11:16.687676, 0:00:12.767692s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 425/478 | 88.91%\n",
      "[1:30:36.344117<0:11:03.591292, 0:00:12.761371s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 426/478 | 89.12%\n",
      "[1:30:52.731031<0:10:51.262962, 0:00:12.769862s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 427/478 | 89.33%\n",
      "[1:31:05.887129<0:10:38.538200, 0:00:12.770764s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 428/478 | 89.54%\n",
      "[1:31:16.760052<0:10:25.550660, 0:00:12.766340s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 429/478 | 89.75%\n",
      "[1:31:28.299561<0:10:12.647376, 0:00:12.763487s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 430/478 | 89.96%\n",
      "[1:31:43.653185<0:10:00.166359, 0:00:12.769497s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 431/478 | 90.17%\n",
      "[1:31:55.579238<0:09:47.307070, 0:00:12.767545s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 432/478 | 90.38%\n",
      "[1:32:05.184888<0:09:34.210890, 0:00:12.760242s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 433/478 | 90.59%\n",
      "[1:32:15.274621<0:09:21.179916, 0:00:12.754089s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 434/478 | 90.79%\n",
      "[1:32:30.349614<0:09:08.655232, 0:00:12.759424s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 435/478 | 91.00%\n",
      "[1:32:44.474869<0:08:56.027394, 0:00:12.762557s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 436/478 | 91.21%\n",
      "[1:32:56.535976<0:08:43.199032, 0:00:12.760952s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 437/478 | 91.42%\n",
      "[1:33:08.748029<0:08:30.387960, 0:00:12.759699s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 438/478 | 91.63%\n",
      "[1:33:22.195323<0:08:17.689335, 0:00:12.761265s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 439/478 | 91.84%\n",
      "[1:33:36.792410<0:08:05.086606, 0:00:12.765437s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 440/478 | 92.05%\n",
      "[1:33:51.207808<0:07:52.459623, 0:00:12.769179s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 441/478 | 92.26%\n",
      "[1:34:06.180595<0:07:39.869904, 0:00:12.774164s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 442/478 | 92.47%\n",
      "[1:34:24.416055<0:07:27.527220, 0:00:12.786492s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 443/478 | 92.68%\n",
      "[1:34:33.969166<0:07:14.493140, 0:00:12.779210s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 444/478 | 92.89%\n",
      "[1:34:43.556453<0:07:01.477221, 0:00:12.772037s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 445/478 | 93.10%\n",
      "[1:34:56.371523<0:06:48.708256, 0:00:12.772133s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 446/478 | 93.31%\n",
      "[1:35:06.924610<0:06:35.782239, 0:00:12.767169s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 447/478 | 93.51%\n",
      "[1:35:19.778199<0:06:23.020860, 0:00:12.767362s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 448/478 | 93.72%\n",
      "[1:35:32.082046<0:06:10.223570, 0:00:12.766330s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 449/478 | 93.93%\n",
      "[1:35:44.789726<0:05:57.453572, 0:00:12.766199s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 450/478 | 94.14%\n",
      "[1:35:57.366355<0:05:44.676033, 0:00:12.765779s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 451/478 | 94.35%\n",
      "[1:36:09.630860<0:05:31.881420, 0:00:12.764670s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 452/478 | 94.56%\n",
      "[1:36:21.346168<0:05:19.058850, 0:00:12.762354s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 453/478 | 94.77%\n",
      "[1:36:32.721900<0:05:06.223176, 0:00:12.759299s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 454/478 | 94.98%\n",
      "[1:36:47.537519<0:04:53.567837, 0:00:12.763819s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 455/478 | 95.19%\n",
      "[1:37:00.071995<0:04:40.792952, 0:00:12.763316s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 456/478 | 95.40%\n",
      "[1:37:13.267870<0:04:28.049502, 0:00:12.764262s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 457/478 | 95.61%\n",
      "[1:37:25.506649<0:04:15.262300, 0:00:12.763115s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 458/478 | 95.82%\n",
      "[1:37:36.701477<0:04:02.434262, 0:00:12.759698s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 459/478 | 96.03%\n",
      "[1:37:50.354405<0:03:49.709520, 0:00:12.761640s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 460/478 | 96.23%\n",
      "[1:38:06.814207<0:03:37.084254, 0:00:12.769662s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 461/478 | 96.44%\n",
      "[1:38:20.177954<0:03:24.335168, 0:00:12.770948s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 462/478 | 96.65%\n",
      "[1:38:31.190497<0:03:11.507250, 0:00:12.767150s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 463/478 | 96.86%\n",
      "[1:38:45.142477<0:02:58.775856, 0:00:12.769704s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 464/478 | 97.07%\n",
      "[1:38:58.048667<0:02:46.009961, 0:00:12.769997s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 465/478 | 97.28%\n",
      "[1:39:11.002237<0:02:33.244692, 0:00:12.770391s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 466/478 | 97.49%\n",
      "[1:39:21.413995<0:02:20.418740, 0:00:12.765340s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 467/478 | 97.70%\n",
      "[1:39:35.643634<0:02:07.684690, 0:00:12.768469s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 468/478 | 97.91%\n",
      "[1:39:48.998305<0:01:54.927471, 0:00:12.769719s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 469/478 | 98.12%\n",
      "[1:39:58.649679<0:01:42.104672, 0:00:12.763084s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 470/478 | 98.33%\n",
      "[1:40:09.266569<0:01:29.309696, 0:00:12.758528s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 471/478 | 98.54%\n",
      "[1:40:20.313349<0:01:16.529406, 0:00:12.754901s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 472/478 | 98.74%\n",
      "[1:40:34.351405<0:01:03.788070, 0:00:12.757614s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 473/478 | 98.95%\n",
      "[1:40:43.989558<0:00:51.004132, 0:00:12.751033s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 474/478 | 99.16%\n",
      "[1:40:56.812734<0:00:38.253555, 0:00:12.751185s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 475/478 | 99.37%\n",
      "[1:41:08.357922<0:00:25.497302, 0:00:12.748651s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 476/478 | 99.58%\n",
      "[1:41:21.561990<0:00:12.749606, 0:00:12.749606s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 477/478 | 99.79%\n",
      "[1:41:32.463040<0:00:00, 0:00:12.745739s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 0: 478/478 | 100.00%\n",
      "[0:00:00.000185<0:00:00.088245, 0:00:00.000185s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 1/478 | 0.21%\n",
      "[0:00:17.907036<1:11:01.874568, 0:00:08.953518s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 2/478 | 0.42%\n",
      "[0:00:35.532493<1:33:45.977900, 0:00:11.844164s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 3/478 | 0.63%\n",
      "[0:00:48.181870<1:35:09.551832, 0:00:12.045468s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 4/478 | 0.84%\n",
      "[0:01:00.824319<1:35:53.980672, 0:00:12.164864s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 5/478 | 1.05%\n",
      "[0:01:14.285058<1:37:23.757896, 0:00:12.380843s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 6/478 | 1.26%\n",
      "[0:01:34.231519<1:45:40.435266, 0:00:13.461646s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 7/478 | 1.46%\n",
      "[0:01:53.378202<1:51:00.969250, 0:00:14.172275s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 8/478 | 1.67%\n",
      "[0:02:14.515479<1:56:49.750916, 0:00:14.946164s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 9/478 | 1.88%\n",
      "[0:02:35.501799<2:01:17.484240, 0:00:15.550180s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 10/478 | 2.09%\n",
      "[0:02:53.645878<2:02:52.056863, 0:00:15.785989s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 11/478 | 2.30%\n",
      "[0:03:09.636825<2:02:44.230154, 0:00:15.803069s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 12/478 | 2.51%\n",
      "[0:03:28.511320<2:04:18.289380, 0:00:16.039332s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 13/478 | 2.72%\n",
      "[0:03:44.069182<2:03:46.293088, 0:00:16.004942s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 14/478 | 2.93%\n",
      "[0:04:02.705926<2:04:51.522885, 0:00:16.180395s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 15/478 | 3.14%\n",
      "[0:04:20.499865<2:05:21.933804, 0:00:16.281242s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 16/478 | 3.35%\n",
      "[0:04:36.446794<2:04:56.586536, 0:00:16.261576s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 17/478 | 3.56%\n",
      "[0:04:55.876282<2:06:01.282660, 0:00:16.437571s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 18/478 | 3.77%\n",
      "[0:05:13.096897<2:06:03.761856, 0:00:16.478784s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 19/478 | 3.97%\n",
      "[0:05:29.699679<2:05:50.122672, 0:00:16.484984s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 20/478 | 4.18%\n",
      "[0:06:01.731719<2:11:11.971240, 0:00:17.225320s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 21/478 | 4.39%\n",
      "[0:06:21.437729<2:11:46.164024, 0:00:17.338079s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 22/478 | 4.60%\n",
      "[0:06:36.154568<2:10:36.970960, 0:00:17.224112s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 23/478 | 4.81%\n",
      "[0:06:52.964484<2:10:11.911716, 0:00:17.206854s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 24/478 | 5.02%\n",
      "[0:07:15.605817<2:11:33.177549, 0:00:17.424233s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 25/478 | 5.23%\n",
      "[0:07:28.470997<2:09:56.495568, 0:00:17.248884s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 26/478 | 5.44%\n",
      "[0:07:44.039781<2:09:11.183209, 0:00:17.186659s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 27/478 | 5.65%\n",
      "[0:07:56.471787<2:07:37.582500, 0:00:17.016850s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 28/478 | 5.86%\n",
      "[0:08:10.905474<2:06:40.570975, 0:00:16.927775s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 29/478 | 6.07%\n",
      "[0:08:23.706865<2:05:22.022592, 0:00:16.790229s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 30/478 | 6.28%\n",
      "[0:08:44.595671<2:06:04.331127, 0:00:16.922441s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 31/478 | 6.49%\n",
      "[0:09:07.333407<2:07:08.459374, 0:00:17.104169s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 32/478 | 6.69%\n",
      "[0:09:24.378762<2:06:50.562215, 0:00:17.102387s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 33/478 | 6.90%\n",
      "[0:09:39.262657<2:06:04.488828, 0:00:17.037137s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 34/478 | 7.11%\n",
      "[0:09:56.687165<2:05:52.354815, 0:00:17.048205s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 35/478 | 7.32%\n",
      "[0:10:27.909837<2:08:29.337480, 0:00:17.441940s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 36/478 | 7.53%\n",
      "[0:10:46.411684<2:08:24.528426, 0:00:17.470586s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 37/478 | 7.74%\n",
      "[0:11:09.247677<2:09:09.183640, 0:00:17.611781s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 38/478 | 7.95%\n",
      "[0:11:26.763805<2:08:50.494992, 0:00:17.609328s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 39/478 | 8.16%\n",
      "[0:11:45.907972<2:08:49.692162, 0:00:17.647699s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 40/478 | 8.37%\n",
      "[0:12:03.797602<2:08:34.623200, 0:00:17.653600s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 41/478 | 8.58%\n",
      "[0:12:15.933004<2:07:19.685304, 0:00:17.522214s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 42/478 | 8.79%\n",
      "[0:12:31.244042<2:06:39.794520, 0:00:17.470792s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 43/478 | 9.00%\n",
      "[0:12:49.977260<2:06:34.775622, 0:00:17.499483s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 44/478 | 9.21%\n",
      "[0:13:07.265078<2:06:15.239740, 0:00:17.494780s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 45/478 | 9.41%\n",
      "[0:13:20.229485<2:05:15.198576, 0:00:17.396293s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 46/478 | 9.62%\n",
      "[0:13:37.412210<2:04:55.843819, 0:00:17.391749s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 47/478 | 9.83%\n",
      "[0:13:53.943224<2:04:30.741310, 0:00:17.373817s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 48/478 | 10.04%\n",
      "[0:14:06.698684<2:03:32.933385, 0:00:17.279565s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 49/478 | 10.25%\n",
      "[0:14:23.545937<2:03:11.953332, 0:00:17.270919s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 50/478 | 10.46%\n",
      "[0:14:44.538611<2:03:25.842738, 0:00:17.343894s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 51/478 | 10.67%\n",
      "[0:14:57.330888<2:02:31.210638, 0:00:17.256363s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 52/478 | 10.88%\n",
      "[0:15:19.814245<2:02:55.869050, 0:00:17.354986s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 53/478 | 11.09%\n",
      "[0:15:32.217187<2:01:59.631144, 0:00:17.263281s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 54/478 | 11.30%\n",
      "[0:16:00.956802<2:03:10.631466, 0:00:17.471942s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 55/478 | 11.51%\n",
      "[0:16:14.937360<2:02:26.849512, 0:00:17.409596s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 56/478 | 11.72%\n",
      "[0:16:36.506056<2:02:40.158602, 0:00:17.482562s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 57/478 | 11.92%\n",
      "[0:16:57.328366<2:02:46.860480, 0:00:17.540144s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 58/478 | 12.13%\n",
      "[0:17:13.586887<2:02:20.218818, 0:00:17.518422s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 59/478 | 12.34%\n",
      "[0:17:28.165137<2:01:42.217142, 0:00:17.469419s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 60/478 | 12.55%\n",
      "[0:17:46.741220<2:01:32.312937, 0:00:17.487561s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 61/478 | 12.76%\n",
      "[0:18:08.850849<2:01:45.837760, 0:00:17.562110s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 62/478 | 12.97%\n",
      "[0:18:27.051337<2:01:32.480845, 0:00:17.572243s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 63/478 | 13.18%\n",
      "[0:18:52.498006<2:02:05.846334, 0:00:17.695281s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 64/478 | 13.39%\n",
      "[0:19:13.140066<2:02:06.874408, 0:00:17.740616s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 65/478 | 13.60%\n",
      "[0:19:31.880277<2:01:55.373944, 0:00:17.755762s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 66/478 | 13.81%\n",
      "[0:19:52.214876<2:01:53.437572, 0:00:17.794252s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 67/478 | 14.02%\n",
      "[0:20:24.645437<2:03:03.891720, 0:00:18.009492s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 68/478 | 14.23%\n",
      "[0:20:54.059735<2:03:53.484611, 0:00:18.174779s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 69/478 | 14.44%\n",
      "[0:21:13.180914<2:03:40.825992, 0:00:18.188299s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 70/478 | 14.64%\n",
      "[0:21:32.856415<2:03:31.162715, 0:00:18.209245s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 71/478 | 14.85%\n",
      "[0:21:53.815223<2:03:28.458204, 0:00:18.247434s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 72/478 | 15.06%\n",
      "[0:22:13.136406<2:03:16.167915, 0:00:18.262143s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 73/478 | 15.27%\n",
      "[0:22:25.964486<2:02:28.238436, 0:00:18.188709s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 74/478 | 15.48%\n",
      "[0:22:40.815301<2:01:52.114212, 0:00:18.144204s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 75/478 | 15.69%\n",
      "[0:22:55.709382<2:01:16.778478, 0:00:18.101439s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 76/478 | 15.90%\n",
      "[0:23:16.015999<2:01:10.161278, 0:00:18.130078s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 77/478 | 16.11%\n",
      "[0:23:31.718526<2:00:39.582000, 0:00:18.098955s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 78/478 | 16.32%\n",
      "[0:23:47.072393<2:00:07.618992, 0:00:18.064208s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 79/478 | 16.53%\n",
      "[0:24:08.013826<2:00:03.868854, 0:00:18.100173s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 80/478 | 16.74%\n",
      "[0:24:26.093417<1:59:45.667843, 0:00:18.099919s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 81/478 | 16.95%\n",
      "[0:24:46.772024<1:59:40.020936, 0:00:18.131366s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 82/478 | 17.15%\n",
      "[0:25:00.681844<1:59:01.799080, 0:00:18.080504s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 83/478 | 17.36%\n",
      "[0:25:17.761325<1:58:39.023278, 0:00:18.068587s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 84/478 | 17.57%\n",
      "[0:25:30.267174<1:57:55.235199, 0:00:18.003143s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 85/478 | 17.78%\n",
      "[0:25:43.140157<1:57:13.848080, 0:00:17.943490s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 86/478 | 17.99%\n",
      "[0:25:55.862523<1:56:32.439507, 0:00:17.883477s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 87/478 | 18.20%\n",
      "[0:26:14.151455<1:56:16.353150, 0:00:17.888085s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 88/478 | 18.41%\n",
      "[0:26:27.585680<1:55:38.997949, 0:00:17.838041s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 89/478 | 18.62%\n",
      "[0:26:40.608677<1:55:00.401908, 0:00:17.784541s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 90/478 | 18.83%\n",
      "[0:26:52.673609<1:54:18.293256, 0:00:17.721688s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 91/478 | 19.04%\n",
      "[0:27:05.259798<1:53:39.024662, 0:00:17.665867s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 92/478 | 19.25%\n",
      "[0:27:24.403190<1:53:27.475675, 0:00:17.681755s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 93/478 | 19.46%\n",
      "[0:27:43.103379<1:53:13.954176, 0:00:17.692589s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 94/478 | 19.67%\n",
      "[0:27:59.032466<1:52:49.151958, 0:00:17.674026s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 95/478 | 19.87%\n",
      "[0:28:18.840838<1:52:39.970938, 0:00:17.696259s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 96/478 | 20.08%\n",
      "[0:28:34.955033<1:52:16.060569, 0:00:17.679949s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 97/478 | 20.29%\n",
      "[0:28:56.850718<1:52:14.727460, 0:00:17.722967s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 98/478 | 20.50%\n",
      "[0:29:13.564822<1:51:53.142104, 0:00:17.712776s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 99/478 | 20.71%\n",
      "[0:29:27.253618<1:51:20.218608, 0:00:17.672536s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 100/478 | 20.92%\n",
      "[0:29:46.279092<1:51:07.596364, 0:00:17.685932s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 101/478 | 21.13%\n",
      "[0:29:58.799434<1:50:30.868664, 0:00:17.635289s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 102/478 | 21.34%\n",
      "[0:30:15.100429<1:50:08.375250, 0:00:17.622334s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 103/478 | 21.55%\n",
      "[0:30:30.019777<1:49:41.032656, 0:00:17.596344s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 104/478 | 21.76%\n",
      "[0:30:46.528569<1:49:19.572778, 0:00:17.585986s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 105/478 | 21.97%\n",
      "[0:31:07.935009<1:49:15.394416, 0:00:17.622028s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 106/478 | 22.18%\n",
      "[0:31:22.463347<1:48:47.045665, 0:00:17.593115s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 107/478 | 22.38%\n",
      "[0:31:42.131065<1:48:36.560250, 0:00:17.612325s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 108/478 | 22.59%\n",
      "[0:31:58.311529<1:48:14.100372, 0:00:17.599188s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 109/478 | 22.80%\n",
      "[0:32:18.495554<1:48:05.148816, 0:00:17.622687s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 110/478 | 23.01%\n",
      "[0:32:35.865472<1:47:46.690470, 0:00:17.620410s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 111/478 | 23.22%\n",
      "[0:32:55.492090<1:47:35.625852, 0:00:17.638322s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 112/478 | 23.43%\n",
      "[0:33:12.924804<1:47:17.323595, 0:00:17.636503s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 113/478 | 23.64%\n",
      "[0:33:31.021489<1:47:01.156196, 0:00:17.640539s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 114/478 | 23.85%\n",
      "[0:33:53.855944<1:46:59.910552, 0:00:17.685704s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 115/478 | 24.06%\n",
      "[0:34:12.026028<1:46:43.736560, 0:00:17.689880s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 116/478 | 24.27%\n",
      "[0:34:27.780562<1:46:20.075018, 0:00:17.673338s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 117/478 | 24.48%\n",
      "[0:34:44.800237<1:46:00.407640, 0:00:17.667799s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 118/478 | 24.69%\n",
      "[0:35:04.046870<1:45:47.502694, 0:00:17.681066s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 119/478 | 24.90%\n",
      "[0:35:19.904666<1:45:24.382176, 0:00:17.665872s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 120/478 | 25.10%\n",
      "[0:35:34.443196<1:44:57.489282, 0:00:17.640026s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 121/478 | 25.31%\n",
      "[0:35:50.542797<1:44:35.354400, 0:00:17.627400s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 122/478 | 25.52%\n",
      "[0:36:07.674013<1:44:16.294930, 0:00:17.623366s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 123/478 | 25.73%\n",
      "[0:36:20.517217<1:43:45.024864, 0:00:17.584816s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 124/478 | 25.94%\n",
      "[0:36:38.671328<1:43:29.047963, 0:00:17.589371s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 125/478 | 26.15%\n",
      "[0:36:57.550461<1:43:15.061664, 0:00:17.599607s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 126/478 | 26.36%\n",
      "[0:37:15.193561<1:42:57.582099, 0:00:17.599949s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 127/478 | 26.57%\n",
      "[0:37:39.178136<1:42:57.440150, 0:00:17.649829s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 128/478 | 26.78%\n",
      "[0:38:07.906886<1:43:09.763488, 0:00:17.735712s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 129/478 | 26.99%\n",
      "[0:38:25.790032<1:42:52.422408, 0:00:17.736846s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 130/478 | 27.20%\n",
      "[0:38:44.457921<1:42:37.152038, 0:00:17.743954s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 131/478 | 27.41%\n",
      "[0:39:04.408768<1:42:25.192512, 0:00:17.760672s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 132/478 | 27.62%\n",
      "[0:39:24.529796<1:42:13.554900, 0:00:17.778420s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 133/478 | 27.82%\n",
      "[0:39:37.095085<1:41:42.393504, 0:00:17.739516s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 134/478 | 28.03%\n",
      "[0:39:52.173819<1:41:17.893458, 0:00:17.719806s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 135/478 | 28.24%\n",
      "[0:40:17.023280<1:41:18.102660, 0:00:17.772230s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 136/478 | 28.45%\n",
      "[0:40:36.961886<1:41:05.722663, 0:00:17.788043s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 137/478 | 28.66%\n",
      "[0:40:50.515559<1:40:37.502060, 0:00:17.757359s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 138/478 | 28.87%\n",
      "[0:41:03.278928<1:40:07.565109, 0:00:17.721431s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 139/478 | 29.08%\n",
      "[0:41:23.361867<1:39:55.545062, 0:00:17.738299s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 140/478 | 29.29%\n",
      "[0:41:44.407566<1:39:45.711772, 0:00:17.761756s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 141/478 | 29.50%\n",
      "[0:41:59.530008<1:39:21.704784, 0:00:17.743169s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 142/478 | 29.71%\n",
      "[0:42:17.184213<1:39:03.753245, 0:00:17.742547s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 143/478 | 29.92%\n",
      "[0:42:29.652644<1:38:33.777614, 0:00:17.705921s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 144/478 | 30.13%\n",
      "[0:42:42.316936<1:38:04.493283, 0:00:17.671151s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 145/478 | 30.33%\n",
      "[0:42:58.038736<1:37:42.389600, 0:00:17.657800s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 146/478 | 30.54%\n",
      "[0:43:10.722084<1:37:13.530760, 0:00:17.623960s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 147/478 | 30.75%\n",
      "[0:43:25.903966<1:36:50.461470, 0:00:17.607459s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 148/478 | 30.96%\n",
      "[0:43:40.236050<1:36:25.621933, 0:00:17.585477s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 149/478 | 31.17%\n",
      "[0:43:57.144879<1:36:06.556848, 0:00:17.580966s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 150/478 | 31.38%\n",
      "[0:44:17.601701<1:35:55.203597, 0:00:17.600011s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 151/478 | 31.59%\n",
      "[0:44:39.715529<1:35:47.284482, 0:00:17.629707s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 152/478 | 31.80%\n",
      "[0:45:05.610204<1:35:47.211275, 0:00:17.683727s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 153/478 | 32.01%\n",
      "[0:45:25.361607<1:35:33.877572, 0:00:17.697153s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 154/478 | 32.22%\n",
      "[0:45:46.665663<1:35:23.696952, 0:00:17.720424s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 155/478 | 32.43%\n",
      "[0:46:09.466747<1:35:16.463424, 0:00:17.752992s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 156/478 | 32.64%\n",
      "[0:46:33.776444<1:35:12.116034, 0:00:17.794754s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 157/478 | 32.85%\n",
      "[0:46:55.851106<1:35:02.989440, 0:00:17.821842s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 158/478 | 33.05%\n",
      "[0:47:09.637082<1:34:37.070740, 0:00:17.796460s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 159/478 | 33.26%\n",
      "[0:47:26.311182<1:34:17.043510, 0:00:17.789445s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 160/478 | 33.47%\n",
      "[0:47:49.611609<1:34:10.104975, 0:00:17.823675s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 161/478 | 33.68%\n",
      "[0:48:02.234480<1:33:42.136436, 0:00:17.791571s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 162/478 | 33.89%\n",
      "[0:48:25.173533<1:33:34.292565, 0:00:17.823151s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 163/478 | 34.10%\n",
      "[0:48:41.613802<1:33:13.821452, 0:00:17.814718s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 164/478 | 34.31%\n",
      "[0:49:00.574723<1:32:58.181145, 0:00:17.821665s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 165/478 | 34.52%\n",
      "[0:49:12.169962<1:32:28.656672, 0:00:17.784156s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 166/478 | 34.73%\n",
      "[0:49:26.674900<1:32:04.765720, 0:00:17.764520s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 167/478 | 34.94%\n",
      "[0:49:40.026968<1:31:38.859360, 0:00:17.738256s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 168/478 | 35.15%\n",
      "[0:49:58.838272<1:31:23.082945, 0:00:17.744605s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 169/478 | 35.36%\n",
      "[0:50:11.958274<1:30:56.959816, 0:00:17.717402s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 170/478 | 35.56%\n",
      "[0:50:25.545976<1:30:31.828057, 0:00:17.693251s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 171/478 | 35.77%\n",
      "[0:50:45.757700<1:30:18.615564, 0:00:17.707894s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 172/478 | 35.98%\n",
      "[0:51:10.594138<1:30:13.475195, 0:00:17.749099s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 173/478 | 36.19%\n",
      "[0:51:24.988353<1:29:49.864672, 0:00:17.729818s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 174/478 | 36.40%\n",
      "[0:51:40.385512<1:29:28.096167, 0:00:17.716489s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 175/478 | 36.61%\n",
      "[0:51:58.472892<1:29:11.015992, 0:00:17.718596s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 176/478 | 36.82%\n",
      "[0:52:12.220098<1:28:46.543859, 0:00:17.696159s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 177/478 | 37.03%\n",
      "[0:52:31.792240<1:28:32.009400, 0:00:17.706698s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 178/478 | 37.24%\n",
      "[0:52:45.802825<1:28:08.128651, 0:00:17.686049s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 179/478 | 37.45%\n",
      "[0:53:04.643677<1:27:52.354570, 0:00:17.692465s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 180/478 | 37.66%\n",
      "[0:53:18.840667<1:27:28.926441, 0:00:17.673153s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 181/478 | 37.87%\n",
      "[0:53:33.212698<1:27:05.884440, 0:00:17.655015s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 182/478 | 38.08%\n",
      "[0:53:49.242929<1:26:45.610120, 0:00:17.646136s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 183/478 | 38.28%\n",
      "[0:54:07.792031<1:26:29.406936, 0:00:17.651044s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 184/478 | 38.49%\n",
      "[0:54:19.313267<1:26:02.047630, 0:00:17.617910s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 185/478 | 38.70%\n",
      "[0:54:36.353386<1:25:43.522476, 0:00:17.614803s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 186/478 | 38.91%\n",
      "[0:54:52.859803<1:25:24.182916, 0:00:17.608876s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 187/478 | 39.12%\n",
      "[0:55:12.306938<1:25:09.409660, 0:00:17.618654s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 188/478 | 39.33%\n",
      "[0:55:31.497704<1:24:54.194908, 0:00:17.626972s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 189/478 | 39.54%\n",
      "[0:55:47.745304<1:24:34.477056, 0:00:17.619712s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 190/478 | 39.75%\n",
      "[0:56:03.047591<1:24:13.375173, 0:00:17.607579s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 191/478 | 39.96%\n",
      "[0:56:16.712308<1:23:49.894298, 0:00:17.587043s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 192/478 | 40.17%\n",
      "[0:56:31.775566<1:23:28.580595, 0:00:17.573967s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 193/478 | 40.38%\n",
      "[0:56:48.817340<1:23:10.227332, 0:00:17.571223s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 194/478 | 40.59%\n",
      "[0:57:05.276620<1:22:51.042443, 0:00:17.565521s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 195/478 | 40.79%\n",
      "[0:57:22.859132<1:22:33.501456, 0:00:17.565608s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 196/478 | 41.00%\n",
      "[0:57:55.339727<1:22:37.210358, 0:00:17.641318s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 197/478 | 41.21%\n",
      "[0:58:13.188086<1:22:19.861920, 0:00:17.642364s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 198/478 | 41.42%\n",
      "[0:58:27.181800<1:21:57.104091, 0:00:17.624029s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 199/478 | 41.63%\n",
      "[0:58:38.821809<1:21:31.162302, 0:00:17.594109s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 200/478 | 41.84%\n",
      "[0:58:56.257492<1:21:13.349917, 0:00:17.593321s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 201/478 | 42.05%\n",
      "[0:59:08.045670<1:20:47.824908, 0:00:17.564583s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 202/478 | 42.26%\n",
      "[0:59:25.756818<1:20:30.458875, 0:00:17.565305s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 203/478 | 42.47%\n",
      "[0:59:39.946064<1:20:08.358870, 0:00:17.548755s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 204/478 | 42.68%\n",
      "[0:59:53.531818<1:19:45.532752, 0:00:17.529424s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 205/478 | 42.89%\n",
      "[1:00:06.961263<1:19:22.589712, 0:00:17.509521s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 206/478 | 43.10%\n",
      "[1:00:22.145434<1:19:02.035777, 0:00:17.498287s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 207/478 | 43.31%\n",
      "[1:00:38.277152<1:18:42.763590, 0:00:17.491717s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 208/478 | 43.51%\n",
      "[1:00:55.500002<1:18:24.925939, 0:00:17.490431s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 209/478 | 43.72%\n",
      "[1:01:07.007497<1:17:59.799920, 0:00:17.461940s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 210/478 | 43.93%\n",
      "[1:01:26.415775<1:17:44.801055, 0:00:17.471165s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 211/478 | 44.14%\n",
      "[1:01:42.120125<1:17:25.113046, 0:00:17.462831s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 212/478 | 44.35%\n",
      "[1:01:57.383483<1:17:04.913825, 0:00:17.452505s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 213/478 | 44.56%\n",
      "[1:02:13.916317<1:16:46.326648, 0:00:17.448207s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 214/478 | 44.77%\n",
      "[1:02:28.283311<1:16:25.109388, 0:00:17.433876s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 215/478 | 44.98%\n",
      "[1:02:39.680090<1:16:00.352612, 0:00:17.405926s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 216/478 | 45.19%\n",
      "[1:02:58.452128<1:15:44.589942, 0:00:17.412222s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 217/478 | 45.40%\n",
      "[1:03:10.349001<1:15:20.599720, 0:00:17.386922s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 218/478 | 45.61%\n",
      "[1:03:21.876952<1:14:56.283771, 0:00:17.360169s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 219/478 | 45.82%\n",
      "[1:03:33.594894<1:14:32.306676, 0:00:17.334522s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 220/478 | 46.03%\n",
      "[1:03:47.456606<1:14:10.933656, 0:00:17.318808s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 221/478 | 46.23%\n",
      "[1:04:03.775511<1:13:52.461824, 0:00:17.314304s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 222/478 | 46.44%\n",
      "[1:04:22.947293<1:13:37.271670, 0:00:17.322634s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 223/478 | 46.65%\n",
      "[1:04:35.914269<1:13:15.010006, 0:00:17.303189s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 224/478 | 46.86%\n",
      "[1:04:49.567455<1:12:53.602398, 0:00:17.286966s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 225/478 | 47.07%\n",
      "[1:05:04.064515<1:12:33.204744, 0:00:17.274622s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 226/478 | 47.28%\n",
      "[1:05:21.061575<1:12:15.623149, 0:00:17.273399s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 227/478 | 47.49%\n",
      "[1:05:34.639373<1:11:54.297500, 0:00:17.257190s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 228/478 | 47.70%\n",
      "[1:05:47.461682<1:11:32.218176, 0:00:17.237824s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 229/478 | 47.91%\n",
      "[1:06:03.583126<1:11:13.776560, 0:00:17.232970s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 230/478 | 48.12%\n",
      "[1:06:23.351613<1:10:59.254662, 0:00:17.243946s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 231/478 | 48.33%\n",
      "[1:06:35.326668<1:10:36.424056, 0:00:17.221236s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 232/478 | 48.54%\n",
      "[1:06:48.636988<1:10:15.090495, 0:00:17.204451s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 233/478 | 48.74%\n",
      "[1:07:06.786560<1:09:58.871560, 0:00:17.208490s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 234/478 | 48.95%\n",
      "[1:07:20.753118<1:09:38.310642, 0:00:17.194694s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 235/478 | 49.16%\n",
      "[1:07:39.444791<1:09:22.650954, 0:00:17.201037s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 236/478 | 49.37%\n",
      "[1:07:57.714516<1:09:06.536586, 0:00:17.205546s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 237/478 | 49.58%\n",
      "[1:08:10.689385<1:08:45.065040, 0:00:17.187771s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 238/478 | 49.79%\n",
      "[1:08:24.744832<1:08:24.744935, 0:00:17.174665s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 239/478 | 50.00%\n",
      "[1:08:40.343699<1:08:06.007562, 0:00:17.168099s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 240/478 | 50.21%\n",
      "[1:08:51.473747<1:07:42.901665, 0:00:17.143045s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 241/478 | 50.42%\n",
      "[1:09:09.776039<1:07:26.889060, 0:00:17.147835s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 242/478 | 50.63%\n",
      "[1:09:23.888422<1:07:06.805605, 0:00:17.135343s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 243/478 | 50.84%\n",
      "[1:09:41.011633<1:06:49.658796, 0:00:17.135294s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 244/478 | 51.05%\n",
      "[1:09:54.709807<1:06:29.254745, 0:00:17.121265s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 245/478 | 51.26%\n",
      "[1:10:15.817041<1:06:15.892576, 0:00:17.137468s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 246/478 | 51.46%\n",
      "[1:10:30.992049<1:05:56.919582, 0:00:17.129522s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 247/478 | 51.67%\n",
      "[1:10:45.183002<1:05:37.064790, 0:00:17.117673s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 248/478 | 51.88%\n",
      "[1:10:58.478674<1:05:16.432196, 0:00:17.102324s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 249/478 | 52.09%\n",
      "[1:11:10.582008<1:04:54.770784, 0:00:17.082328s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 250/478 | 52.30%\n",
      "[1:11:25.027851<1:04:35.304048, 0:00:17.071824s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 251/478 | 52.51%\n",
      "[1:11:38.091757<1:04:14.637920, 0:00:17.055920s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 252/478 | 52.72%\n",
      "[1:11:49.742505<1:03:52.774875, 0:00:17.034555s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 253/478 | 52.93%\n",
      "[1:12:01.299270<1:03:30.909536, 0:00:17.012989s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 254/478 | 53.14%\n",
      "[1:12:19.397725<1:03:14.845858, 0:00:17.017246s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 255/478 | 53.35%\n",
      "[1:12:33.734227<1:02:55.503828, 0:00:17.006774s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 256/478 | 53.56%\n",
      "[1:12:51.668156<1:02:39.294422, 0:00:17.010382s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 257/478 | 53.77%\n",
      "[1:13:11.303005<1:02:24.521880, 0:00:17.020554s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 258/478 | 53.97%\n",
      "[1:13:31.772597<1:02:10.417749, 0:00:17.033871s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 259/478 | 54.18%\n",
      "[1:13:47.042918<1:01:51.905184, 0:00:17.027088s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 260/478 | 54.39%\n",
      "[1:14:04.922450<1:01:35.586818, 0:00:17.030354s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 261/478 | 54.60%\n",
      "[1:14:24.923907<1:01:21.005904, 0:00:17.041694s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 262/478 | 54.81%\n",
      "[1:14:41.147104<1:01:03.295130, 0:00:17.038582s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 263/478 | 55.02%\n",
      "[1:14:58.693120<1:00:46.667856, 0:00:17.040504s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 264/478 | 55.23%\n",
      "[1:15:20.656183<1:00:33.584040, 0:00:17.059080s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 265/478 | 55.44%\n",
      "[1:15:40.799397<1:00:18.982888, 0:00:17.070674s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 266/478 | 55.65%\n",
      "[1:15:54.239450<0:59:59.043247, 0:00:17.057077s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 267/478 | 55.86%\n",
      "[1:16:14.653402<0:59:44.616420, 0:00:17.069602s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 268/478 | 56.07%\n",
      "[1:16:30.538608<0:59:26.626591, 0:00:17.065199s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 269/478 | 56.28%\n",
      "[1:16:44.050673<0:59:06.824320, 0:00:17.052040s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 270/478 | 56.49%\n",
      "[1:16:59.962913<0:58:48.901638, 0:00:17.047834s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 271/478 | 56.69%\n",
      "[1:17:18.125057<0:58:32.697580, 0:00:17.051930s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 272/478 | 56.90%\n",
      "[1:17:31.161824<0:58:12.630715, 0:00:17.037223s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 273/478 | 57.11%\n",
      "[1:17:44.255278<0:57:52.657116, 0:00:17.022829s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 274/478 | 57.32%\n",
      "[1:18:03.497591<0:57:37.272700, 0:00:17.030900s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 275/478 | 57.53%\n",
      "[1:18:18.737746<0:57:18.931224, 0:00:17.024412s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 276/478 | 57.74%\n",
      "[1:18:32.672506<0:56:59.664858, 0:00:17.013258s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 277/478 | 57.95%\n",
      "[1:18:44.087216<0:56:38.623800, 0:00:16.993119s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 278/478 | 58.16%\n",
      "[1:18:55.218438<0:56:17.449691, 0:00:16.972109s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 279/478 | 58.37%\n",
      "[1:19:06.908302<0:55:56.742312, 0:00:16.953244s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 280/478 | 58.58%\n",
      "[1:19:22.989645<0:55:39.177777, 0:00:16.950141s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 281/478 | 58.79%\n",
      "[1:19:38.413468<0:55:21.166884, 0:00:16.944729s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 282/478 | 59.00%\n",
      "[1:19:57.195978<0:55:05.488485, 0:00:16.951223s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 283/478 | 59.21%\n",
      "[1:20:10.732115<0:54:46.204412, 0:00:16.939198s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 284/478 | 59.41%\n",
      "[1:20:30.183112<0:54:30.966123, 0:00:16.948011s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 285/478 | 59.62%\n",
      "[1:20:47.320278<0:54:14.145024, 0:00:16.948672s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 286/478 | 59.83%\n",
      "[1:21:04.807598<0:53:57.554859, 0:00:16.950549s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 287/478 | 60.04%\n",
      "[1:21:16.684002<0:53:37.256890, 0:00:16.932931s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 288/478 | 60.25%\n",
      "[1:21:28.277030<0:53:16.831617, 0:00:16.914453s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 289/478 | 60.46%\n",
      "[1:21:46.583032<0:53:00.819376, 0:00:16.919252s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 290/478 | 60.67%\n",
      "[1:22:05.292798<0:52:45.050735, 0:00:16.925405s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 291/478 | 60.88%\n",
      "[1:22:21.586649<0:52:27.723012, 0:00:16.923242s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 292/478 | 61.09%\n",
      "[1:22:46.635457<0:52:15.930190, 0:00:16.950974s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 293/478 | 61.30%\n",
      "[1:23:04.973275<0:51:59.847144, 0:00:16.955691s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 294/478 | 61.51%\n",
      "[1:23:22.256224<0:51:43.094583, 0:00:16.956801s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 295/478 | 61.72%\n",
      "[1:23:39.544323<0:51:26.341440, 0:00:16.957920s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 296/478 | 61.92%\n",
      "[1:24:03.625528<0:51:13.724624, 0:00:16.981904s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 297/478 | 62.13%\n",
      "[1:24:16.566671<0:50:54.302100, 0:00:16.968345s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 298/478 | 62.34%\n",
      "[1:24:30.616181<0:50:35.586357, 0:00:16.958583s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 299/478 | 62.55%\n",
      "[1:24:46.029375<0:50:17.710718, 0:00:16.953431s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 300/478 | 62.76%\n",
      "[1:25:01.575704<0:49:59.929812, 0:00:16.948756s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 301/478 | 62.97%\n",
      "[1:25:13.121668<0:49:39.832416, 0:00:16.930866s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 302/478 | 63.18%\n",
      "[1:25:24.881422<0:49:19.915000, 0:00:16.913800s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 303/478 | 63.39%\n",
      "[1:25:40.642280<0:49:02.341392, 0:00:16.910008s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 304/478 | 63.60%\n",
      "[1:25:57.943568<0:48:45.653170, 0:00:16.911290s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 305/478 | 63.81%\n",
      "[1:26:16.916004<0:48:29.900472, 0:00:16.918026s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 306/478 | 64.02%\n",
      "[1:26:28.475796<0:48:09.997983, 0:00:16.900573s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 307/478 | 64.23%\n",
      "[1:26:44.056374<0:47:52.368790, 0:00:16.896287s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 308/478 | 64.44%\n",
      "[1:27:00.935841<0:47:35.463208, 0:00:16.896232s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 309/478 | 64.64%\n",
      "[1:27:16.167162<0:47:17.664816, 0:00:16.890862s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 310/478 | 64.85%\n",
      "[1:27:45.916510<0:47:07.678569, 0:00:16.932207s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 311/478 | 65.06%\n",
      "[1:28:02.283667<0:46:50.445736, 0:00:16.930396s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 312/478 | 65.27%\n",
      "[1:28:20.626919<0:46:34.260150, 0:00:16.934910s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 313/478 | 65.48%\n",
      "[1:28:35.771670<0:46:16.390276, 0:00:16.929209s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 314/478 | 65.69%\n",
      "[1:28:59.068824<0:46:02.756275, 0:00:16.949425s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 315/478 | 65.90%\n",
      "[1:29:11.169271<0:45:43.320960, 0:00:16.934080s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 316/478 | 66.11%\n",
      "[1:29:26.665018<0:45:25.656423, 0:00:16.929543s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 317/478 | 66.32%\n",
      "[1:29:43.229452<0:45:08.543200, 0:00:16.928395s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 318/478 | 66.53%\n",
      "[1:29:57.719782<0:44:50.399568, 0:00:16.920752s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 319/478 | 66.74%\n",
      "[1:30:15.382956<0:44:33.845376, 0:00:16.923072s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 320/478 | 66.95%\n",
      "[1:30:30.073159<0:44:15.830212, 0:00:16.916116s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 321/478 | 67.15%\n",
      "[1:31:15.301962<0:44:12.630708, 0:00:17.004043s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 322/478 | 67.36%\n",
      "[1:31:33.356205<0:43:56.130725, 0:00:17.007295s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 323/478 | 67.57%\n",
      "[1:31:48.119471<0:43:38.056826, 0:00:17.000369s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 324/478 | 67.78%\n",
      "[1:32:07.561880<0:43:22.206099, 0:00:17.007883s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 325/478 | 67.99%\n",
      "[1:32:19.279567<0:43:02.731560, 0:00:16.991655s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 326/478 | 68.20%\n",
      "[1:32:35.315073<0:42:45.298381, 0:00:16.988731s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 327/478 | 68.41%\n",
      "[1:32:46.803249<0:42:25.794150, 0:00:16.971961s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 328/478 | 68.62%\n",
      "[1:33:05.495420<0:42:09.601310, 0:00:16.977190s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 329/478 | 68.83%\n",
      "[1:33:23.844556<0:41:53.239356, 0:00:16.981347s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 330/478 | 69.04%\n",
      "[1:33:39.669781<0:41:35.744538, 0:00:16.977854s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 331/478 | 69.25%\n",
      "[1:34:00.330884<0:41:20.386408, 0:00:16.988948s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 332/478 | 69.46%\n",
      "[1:34:15.133698<0:41:02.445535, 0:00:16.982383s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 333/478 | 69.67%\n",
      "[1:34:28.772125<0:40:44.021568, 0:00:16.972372s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 334/478 | 69.87%\n",
      "[1:34:40.711744<0:40:24.900764, 0:00:16.957348s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 335/478 | 70.08%\n",
      "[1:34:54.432011<0:40:06.575388, 0:00:16.947714s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 336/478 | 70.29%\n",
      "[1:35:07.264807<0:39:47.906064, 0:00:16.935504s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 337/478 | 70.50%\n",
      "[1:35:22.909404<0:39:30.435900, 0:00:16.931685s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 338/478 | 70.71%\n",
      "[1:35:41.080857<0:39:14.012538, 0:00:16.935342s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 339/478 | 70.92%\n",
      "[1:36:05.683235<0:39:00.189096, 0:00:16.957892s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 340/478 | 71.13%\n",
      "[1:36:26.750117<0:38:44.882054, 0:00:16.969942s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 341/478 | 71.34%\n",
      "[1:36:47.140473<0:38:29.272248, 0:00:16.979943s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 342/478 | 71.55%\n",
      "[1:37:13.526772<0:38:15.994545, 0:00:17.007367s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 343/478 | 71.76%\n",
      "[1:37:29.267328<0:37:58.493656, 0:00:17.003684s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 344/478 | 71.97%\n",
      "[1:37:42.417296<0:37:40.004362, 0:00:16.992514s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 345/478 | 72.18%\n",
      "[1:38:04.245866<0:37:24.856812, 0:00:17.006491s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 346/478 | 72.38%\n",
      "[1:38:22.530882<0:37:08.332925, 0:00:17.010175s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 347/478 | 72.59%\n",
      "[1:38:37.079277<0:36:50.403130, 0:00:17.003101s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 348/478 | 72.80%\n",
      "[1:38:54.001356<0:36:33.370101, 0:00:17.002869s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 349/478 | 73.01%\n",
      "[1:39:08.774017<0:36:15.551616, 0:00:16.996497s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 350/478 | 73.22%\n",
      "[1:39:25.193067<0:35:58.346204, 0:00:16.994852s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 351/478 | 73.43%\n",
      "[1:39:41.710049<0:35:41.180244, 0:00:16.993494s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 352/478 | 73.64%\n",
      "[1:39:57.557948<0:35:23.781125, 0:00:16.990249s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 353/478 | 73.85%\n",
      "[1:40:09.288591<0:35:04.948608, 0:00:16.975392s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 354/478 | 74.06%\n",
      "[1:40:25.943669<0:34:47.862147, 0:00:16.974489s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 355/478 | 74.27%\n",
      "[1:40:42.040689<0:34:30.586928, 0:00:16.972024s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 356/478 | 74.48%\n",
      "[1:41:01.058135<0:34:14.308234, 0:00:16.977754s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 357/478 | 74.69%\n",
      "[1:41:18.651683<0:33:57.536880, 0:00:16.979474s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 358/478 | 74.90%\n",
      "[1:41:30.430506<0:33:38.833453, 0:00:16.964987s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 359/478 | 75.10%\n",
      "[1:41:49.674605<0:33:22.615524, 0:00:16.971318s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 360/478 | 75.31%\n",
      "[1:42:03.497420<0:33:04.623849, 0:00:16.962597s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 361/478 | 75.52%\n",
      "[1:42:21.101045<0:32:47.866688, 0:00:16.964368s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 362/478 | 75.73%\n",
      "[1:42:33.114547<0:32:29.333835, 0:00:16.950729s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 363/478 | 75.94%\n",
      "[1:42:56.666676<0:32:14.450496, 0:00:16.968864s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 364/478 | 76.15%\n",
      "[1:43:09.586838<0:31:56.228236, 0:00:16.957772s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 365/478 | 76.36%\n",
      "[1:43:22.935531<0:31:38.166032, 0:00:16.947911s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 366/478 | 76.57%\n",
      "[1:43:34.726435<0:31:19.658349, 0:00:16.933859s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 367/478 | 76.78%\n",
      "[1:43:46.266993<0:31:01.112440, 0:00:16.919204s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 368/478 | 76.99%\n",
      "[1:43:57.933876<0:30:42.641730, 0:00:16.904970s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 369/478 | 77.20%\n",
      "[1:44:14.869233<0:30:25.745616, 0:00:16.905052s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 370/478 | 77.41%\n",
      "[1:44:28.378916<0:30:07.861300, 0:00:16.895900s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 371/478 | 77.62%\n",
      "[1:44:43.049319<0:29:50.331308, 0:00:16.889918s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 372/478 | 77.82%\n",
      "[1:45:02.154995<0:29:34.065090, 0:00:16.895858s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 373/478 | 78.03%\n",
      "[1:45:23.423033<0:29:18.384992, 0:00:16.907548s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 374/478 | 78.24%\n",
      "[1:45:42.322234<0:29:02.024477, 0:00:16.912859s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 375/478 | 78.45%\n",
      "[1:46:02.313265<0:28:45.946692, 0:00:16.921046s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 376/478 | 78.66%\n",
      "[1:46:18.311695<0:28:28.778499, 0:00:16.918599s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 377/478 | 78.87%\n",
      "[1:46:31.985922<0:28:11.001600, 0:00:16.910016s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 378/478 | 79.08%\n",
      "[1:46:51.981171<0:27:54.897444, 0:00:16.918156s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 379/478 | 79.29%\n",
      "[1:47:03.689698<0:27:36.635806, 0:00:16.904447s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 380/478 | 79.50%\n",
      "[1:47:26.465182<0:27:21.226032, 0:00:16.919856s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 381/478 | 79.71%\n",
      "[1:47:45.412946<0:27:04.815840, 0:00:16.925165s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 382/478 | 79.92%\n",
      "[1:47:56.902402<0:26:46.542340, 0:00:16.910972s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 383/478 | 80.13%\n",
      "[1:48:13.261609<0:26:29.496290, 0:00:16.909535s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 384/478 | 80.33%\n",
      "[1:48:24.710917<0:26:11.267829, 0:00:16.895353s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 385/478 | 80.54%\n",
      "[1:48:39.433701<0:25:53.854700, 0:00:16.889725s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 386/478 | 80.75%\n",
      "[1:49:00.274175<0:25:37.893903, 0:00:16.899933s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 387/478 | 80.96%\n",
      "[1:49:16.698582<0:25:20.883720, 0:00:16.898708s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 388/478 | 81.17%\n",
      "[1:49:29.209616<0:25:02.981092, 0:00:16.887428s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 389/478 | 81.38%\n",
      "[1:49:41.411604<0:24:45.036432, 0:00:16.875414s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 390/478 | 81.59%\n",
      "[1:49:59.176144<0:24:28.358856, 0:00:16.877688s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 391/478 | 81.80%\n",
      "[1:50:15.609796<0:24:11.383816, 0:00:16.876556s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 392/478 | 82.01%\n",
      "[1:50:32.273426<0:23:54.461190, 0:00:16.876014s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 393/478 | 82.22%\n",
      "[1:50:48.481286<0:23:37.442712, 0:00:16.874318s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 394/478 | 82.43%\n",
      "[1:51:01.208424<0:23:19.696977, 0:00:16.863819s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 395/478 | 82.64%\n",
      "[1:51:14.388738<0:23:02.070394, 0:00:16.854517s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 396/478 | 82.85%\n",
      "[1:51:26.014348<0:22:44.149026, 0:00:16.841346s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 397/478 | 83.05%\n",
      "[1:51:37.669329<0:22:26.265200, 0:00:16.828315s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 398/478 | 83.26%\n",
      "[1:51:58.184056<0:22:10.166766, 0:00:16.837554s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 399/478 | 83.47%\n",
      "[1:52:12.618089<0:21:52.860510, 0:00:16.831545s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 400/478 | 83.68%\n",
      "[1:52:27.954722<0:21:35.741909, 0:00:16.827817s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 401/478 | 83.89%\n",
      "[1:52:41.714378<0:21:18.334060, 0:00:16.820185s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 402/478 | 84.10%\n",
      "[1:53:00.477027<0:21:01.875375, 0:00:16.825005s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 403/478 | 84.31%\n",
      "[1:53:15.915503<0:20:44.796402, 0:00:16.821573s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 404/478 | 84.52%\n",
      "[1:53:28.388350<0:20:27.190955, 0:00:16.810835s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 405/478 | 84.73%\n",
      "[1:53:50.393067<0:20:11.301216, 0:00:16.823628s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 406/478 | 84.94%\n",
      "[1:54:05.009491<0:19:54.092555, 0:00:16.818205s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 407/478 | 85.15%\n",
      "[1:54:16.456098<0:19:36.352730, 0:00:16.805039s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 408/478 | 85.36%\n",
      "[1:54:32.519899<0:19:19.422663, 0:00:16.803227s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 409/478 | 85.56%\n",
      "[1:54:46.175729<0:19:02.097468, 0:00:16.795551s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 410/478 | 85.77%\n",
      "[1:54:57.816987<0:18:44.461670, 0:00:16.783010s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 411/478 | 85.98%\n",
      "[1:55:11.313769<0:18:27.152178, 0:00:16.775033s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 412/478 | 86.19%\n",
      "[1:55:35.392095<0:18:11.526605, 0:00:16.792717s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 413/478 | 86.40%\n",
      "[1:55:46.916026<0:17:53.919360, 0:00:16.779990s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 414/478 | 86.61%\n",
      "[1:56:04.880883<0:17:37.319298, 0:00:16.782846s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 415/478 | 86.82%\n",
      "[1:56:25.512063<0:17:21.109952, 0:00:16.792096s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 416/478 | 87.03%\n",
      "[1:56:43.573659<0:17:04.503601, 0:00:16.795141s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 417/478 | 87.24%\n",
      "[1:57:01.171292<0:16:47.823660, 0:00:16.797061s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 418/478 | 87.45%\n",
      "[1:57:18.291879<0:16:31.072147, 0:00:16.797833s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 419/478 | 87.66%\n",
      "[1:57:38.847406<0:16:14.793240, 0:00:16.806780s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 420/478 | 87.87%\n",
      "[1:57:50.522228<0:15:57.291630, 0:00:16.794590s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 421/478 | 88.08%\n",
      "[1:58:04.057474<0:15:40.064496, 0:00:16.786866s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 422/478 | 88.28%\n",
      "[1:58:15.690429<0:15:22.607510, 0:00:16.774682s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 423/478 | 88.49%\n",
      "[1:58:27.277180<0:15:05.172084, 0:00:16.762446s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 424/478 | 88.70%\n",
      "[1:58:40.381794<0:14:47.953520, 0:00:16.753840s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 425/478 | 88.91%\n",
      "[1:58:52.103896<0:14:30.585456, 0:00:16.742028s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 426/478 | 89.12%\n",
      "[1:59:12.087518<0:14:14.230569, 0:00:16.749619s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 427/478 | 89.33%\n",
      "[1:59:25.624882<0:13:57.105700, 0:00:16.742114s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 428/478 | 89.54%\n",
      "[1:59:37.148893<0:13:39.767599, 0:00:16.729951s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 429/478 | 89.75%\n",
      "[1:59:48.964020<0:13:22.489008, 0:00:16.718521s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 430/478 | 89.96%\n",
      "[2:00:05.532696<0:13:05.754131, 0:00:16.718173s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 431/478 | 90.17%\n",
      "[2:00:16.972484<0:12:48.473930, 0:00:16.705955s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 432/478 | 90.38%\n",
      "[2:00:28.541790<0:12:31.234140, 0:00:16.694092s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 433/478 | 90.59%\n",
      "[2:00:39.934663<0:12:14.002588, 0:00:16.681877s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 434/478 | 90.79%\n",
      "[2:00:55.848642<0:11:57.244816, 0:00:16.680112s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 435/478 | 91.00%\n",
      "[2:01:14.554360<0:11:40.759836, 0:00:16.684758s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 436/478 | 91.21%\n",
      "[2:01:30.046055<0:11:23.963148, 0:00:16.682028s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 437/478 | 91.42%\n",
      "[2:01:51.525588<0:11:07.719240, 0:00:16.692981s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 438/478 | 91.63%\n",
      "[2:02:05.375835<0:10:50.773695, 0:00:16.686505s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 439/478 | 91.84%\n",
      "[2:02:25.144155<0:10:34.353342, 0:00:16.693509s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 440/478 | 92.05%\n",
      "[2:02:43.376486<0:10:17.788963, 0:00:16.696999s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 441/478 | 92.26%\n",
      "[2:03:00.290598<0:10:01.109640, 0:00:16.697490s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 442/478 | 92.47%\n",
      "[2:03:21.612697<0:09:44.777515, 0:00:16.707929s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 443/478 | 92.68%\n",
      "[2:03:33.354390<0:09:27.689296, 0:00:16.696744s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 444/478 | 92.89%\n",
      "[2:03:44.325498<0:09:10.567974, 0:00:16.683878s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 445/478 | 93.10%\n",
      "[2:04:01.285909<0:08:53.903936, 0:00:16.684498s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 446/478 | 93.31%\n",
      "[2:04:12.910918<0:08:36.868549, 0:00:16.673179s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 447/478 | 93.51%\n",
      "[2:04:29.172895<0:08:20.167830, 0:00:16.672261s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 448/478 | 93.72%\n",
      "[2:04:47.847077<0:08:03.624880, 0:00:16.676720s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 449/478 | 93.93%\n",
      "[2:05:05.176722<0:07:46.988760, 0:00:16.678170s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 450/478 | 94.14%\n",
      "[2:05:23.722518<0:07:30.422424, 0:00:16.682312s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 451/478 | 94.35%\n",
      "[2:05:37.653518<0:07:13.581850, 0:00:16.676225s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 452/478 | 94.56%\n",
      "[2:05:51.580070<0:06:56.753875, 0:00:16.670155s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 453/478 | 94.77%\n",
      "[2:06:03.500049<0:06:39.832608, 0:00:16.659692s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 454/478 | 94.98%\n",
      "[2:06:18.348869<0:06:23.081376, 0:00:16.655712s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 455/478 | 95.19%\n",
      "[2:06:31.542401<0:06:06.258618, 0:00:16.648119s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 456/478 | 95.40%\n",
      "[2:06:46.055482<0:05:49.512387, 0:00:16.643447s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 457/478 | 95.61%\n",
      "[2:07:01.141503<0:05:32.800940, 0:00:16.640047s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 458/478 | 95.82%\n",
      "[2:07:18.293871<0:05:16.182097, 0:00:16.641163s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 459/478 | 96.03%\n",
      "[2:07:36.865897<0:04:59.616498, 0:00:16.645361s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 460/478 | 96.23%\n",
      "[2:07:56.320083<0:04:43.074718, 0:00:16.651454s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 461/478 | 96.44%\n",
      "[2:08:14.828573<0:04:26.487568, 0:00:16.655473s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 462/478 | 96.65%\n",
      "[2:08:26.634001<0:04:09.674970, 0:00:16.644998s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 463/478 | 96.86%\n",
      "[2:08:44.258168<0:03:53.059512, 0:00:16.647108s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 464/478 | 97.07%\n",
      "[2:09:03.219955<0:03:36.477118, 0:00:16.652086s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 465/478 | 97.28%\n",
      "[2:09:20.888009<0:03:19.851192, 0:00:16.654266s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 466/478 | 97.49%\n",
      "[2:09:36.901476<0:03:03.181834, 0:00:16.652894s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 467/478 | 97.70%\n",
      "[2:09:54.288794<0:02:46.544630, 0:00:16.654463s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 468/478 | 97.91%\n",
      "[2:10:08.449908<0:02:29.842323, 0:00:16.649147s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 469/478 | 98.12%\n",
      "[2:10:25.093451<0:02:13.193080, 0:00:16.649135s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 470/478 | 98.33%\n",
      "[2:10:38.772961<0:01:56.499810, 0:00:16.642830s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 471/478 | 98.54%\n",
      "[2:10:54.256259<0:01:39.842238, 0:00:16.640373s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 472/478 | 98.74%\n",
      "[2:11:08.799334<0:01:23.179695, 0:00:16.635939s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 473/478 | 98.95%\n",
      "[2:11:20.446315<0:01:06.501656, 0:00:16.625414s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 474/478 | 99.16%\n",
      "[2:11:34.902347<0:00:49.862541, 0:00:16.620847s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 475/478 | 99.37%\n",
      "[2:11:46.903243<0:00:33.222282, 0:00:16.611141s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 476/478 | 99.58%\n",
      "[2:12:06.999204<0:00:16.618447, 0:00:16.618447s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 477/478 | 99.79%\n",
      "[2:12:20.803679<0:00:00, 0:00:16.612560s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 1: 478/478 | 100.00%\n",
      "[0:00:00.000190<0:00:00.090630, 0:00:00.000190s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 1/478 | 0.21%\n",
      "[0:00:11.573689<0:45:54.537744, 0:00:05.786844s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 2/478 | 0.42%\n",
      "[0:00:22.571204<0:59:33.774125, 0:00:07.523735s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 3/478 | 0.63%\n",
      "[0:00:32.202265<1:03:35.968284, 0:00:08.050566s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 4/478 | 0.84%\n",
      "[0:00:42.083970<1:06:21.143562, 0:00:08.416794s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 5/478 | 1.05%\n",
      "[0:00:54.964213<1:12:03.851344, 0:00:09.160702s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 6/478 | 1.26%\n",
      "[0:01:06.115647<1:14:08.638332, 0:00:09.445092s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 7/478 | 1.46%\n",
      "[0:01:17.773840<1:16:09.213100, 0:00:09.721730s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 8/478 | 1.67%\n",
      "[0:01:29.717509<1:17:55.279028, 0:00:09.968612s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 9/478 | 1.88%\n",
      "[0:01:40.418704<1:18:19.595160, 0:00:10.041870s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 10/478 | 2.09%\n",
      "[0:01:52.297664<1:19:27.546493, 0:00:10.208879s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 11/478 | 2.30%\n",
      "[0:02:04.212946<1:20:23.602814, 0:00:10.351079s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 12/478 | 2.51%\n",
      "[0:02:19.167170<1:22:57.902655, 0:00:10.705167s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 13/478 | 2.72%\n",
      "[0:02:31.192191<1:23:30.941088, 0:00:10.799442s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 14/478 | 2.93%\n",
      "[0:02:42.385052<1:23:32.285210, 0:00:10.825670s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 15/478 | 3.14%\n",
      "[0:02:54.894051<1:24:10.065636, 0:00:10.930878s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 16/478 | 3.35%\n",
      "[0:03:08.191688<1:25:03.315639, 0:00:11.070099s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 17/478 | 3.56%\n",
      "[0:03:23.678562<1:26:45.118960, 0:00:11.315476s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 18/478 | 3.77%\n",
      "[0:03:36.851771<1:27:18.682209, 0:00:11.413251s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 19/478 | 3.97%\n",
      "[0:03:49.639055<1:27:38.734474, 0:00:11.481953s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 20/478 | 4.18%\n",
      "[0:04:00.304368<1:27:09.480705, 0:00:11.443065s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 21/478 | 4.39%\n",
      "[0:04:12.527724<1:27:14.211048, 0:00:11.478533s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 22/478 | 4.60%\n",
      "[0:04:25.523858<1:27:32.754780, 0:00:11.544516s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 23/478 | 4.81%\n",
      "[0:04:37.568883<1:27:30.677980, 0:00:11.565370s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 24/478 | 5.02%\n",
      "[0:04:48.934958<1:27:15.501294, 0:00:11.557398s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 25/478 | 5.23%\n",
      "[0:04:58.656647<1:26:32.030888, 0:00:11.486794s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 26/478 | 5.44%\n",
      "[0:05:10.713898<1:26:30.072822, 0:00:11.507922s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 27/478 | 5.65%\n",
      "[0:05:20.505278<1:25:50.977650, 0:00:11.446617s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 28/478 | 5.86%\n",
      "[0:05:31.037284<1:25:25.370471, 0:00:11.415079s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 29/478 | 6.07%\n",
      "[0:05:40.856487<1:24:50.123584, 0:00:11.361883s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 30/478 | 6.28%\n",
      "[0:05:52.600380<1:24:44.270082, 0:00:11.374206s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 31/478 | 6.49%\n",
      "[0:06:04.431219<1:24:39.260296, 0:00:11.388476s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 32/478 | 6.69%\n",
      "[0:06:15.925740<1:24:29.301605, 0:00:11.391689s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 33/478 | 6.90%\n",
      "[0:06:27.739900<1:24:23.427060, 0:00:11.404115s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 34/478 | 7.11%\n",
      "[0:06:40.886744<1:24:34.080801, 0:00:11.453907s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 35/478 | 7.32%\n",
      "[0:06:56.395174<1:25:12.407586, 0:00:11.566533s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 36/478 | 7.53%\n",
      "[0:07:06.957516<1:24:48.871872, 0:00:11.539392s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 37/478 | 7.74%\n",
      "[0:07:20.906826<1:25:05.236840, 0:00:11.602811s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 38/478 | 7.95%\n",
      "[0:07:31.540178<1:24:42.721367, 0:00:11.577953s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 39/478 | 8.16%\n",
      "[0:07:43.871680<1:24:39.394896, 0:00:11.596792s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 40/478 | 8.37%\n",
      "[0:07:55.437230<1:24:27.465110, 0:00:11.596030s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 41/478 | 8.58%\n",
      "[0:08:05.524301<1:24:00.204472, 0:00:11.560102s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 42/478 | 8.79%\n",
      "[0:08:16.687011<1:23:44.624535, 0:00:11.550861s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 43/478 | 9.00%\n",
      "[0:08:28.987581<1:23:40.468600, 0:00:11.567900s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 44/478 | 9.21%\n",
      "[0:08:41.637623<1:23:39.313051, 0:00:11.591947s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 45/478 | 9.41%\n",
      "[0:08:51.689468<1:23:13.257744, 0:00:11.558467s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 46/478 | 9.62%\n",
      "[0:09:00.453372<1:22:36.072448, 0:00:11.499008s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 47/478 | 9.83%\n",
      "[0:09:10.469736<1:22:11.291600, 0:00:11.468120s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 48/478 | 10.04%\n",
      "[0:09:20.221555<1:21:44.796897, 0:00:11.433093s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 49/478 | 10.25%\n",
      "[0:09:32.139058<1:21:37.510268, 0:00:11.442781s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 50/478 | 10.46%\n",
      "[0:09:44.300231<1:21:32.082209, 0:00:11.456867s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 51/478 | 10.67%\n",
      "[0:09:53.434076<1:21:01.594644, 0:00:11.412194s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 52/478 | 10.88%\n",
      "[0:10:04.638345<1:20:48.515175, 0:00:11.408271s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 53/478 | 11.09%\n",
      "[0:10:14.054554<1:20:21.465544, 0:00:11.371381s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 54/478 | 11.30%\n",
      "[0:10:26.996007<1:20:22.169121, 0:00:11.399927s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 55/478 | 11.51%\n",
      "[0:10:38.395455<1:20:10.765818, 0:00:11.399919s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 56/478 | 11.72%\n",
      "[0:10:50.638581<1:20:05.593752, 0:00:11.414712s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 57/478 | 11.92%\n",
      "[0:11:03.695813<1:20:06.073020, 0:00:11.443031s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 58/478 | 12.13%\n",
      "[0:11:14.263371<1:19:48.412867, 0:00:11.428193s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 59/478 | 12.34%\n",
      "[0:11:26.462158<1:19:42.353048, 0:00:11.441036s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 60/478 | 12.55%\n",
      "[0:11:39.017093<1:19:38.526849, 0:00:11.459297s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 61/478 | 12.76%\n",
      "[0:11:51.071502<1:19:31.060320, 0:00:11.468895s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 62/478 | 12.97%\n",
      "[0:12:03.397760<1:19:25.239160, 0:00:11.482504s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 63/478 | 13.18%\n",
      "[0:12:15.303034<1:19:16.491540, 0:00:11.489110s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 64/478 | 13.39%\n",
      "[0:12:27.906542<1:19:12.082902, 0:00:11.506254s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 65/478 | 13.60%\n",
      "[0:12:40.925815<1:19:10.021748, 0:00:11.529179s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 66/478 | 13.81%\n",
      "[0:12:52.323319<1:18:57.684954, 0:00:11.527214s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 67/478 | 14.02%\n",
      "[0:13:07.396523<1:19:07.538010, 0:00:11.579361s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 68/478 | 14.23%\n",
      "[0:13:20.046501<1:19:02.304693, 0:00:11.594877s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 69/478 | 14.44%\n",
      "[0:13:33.475863<1:19:01.402272, 0:00:11.621084s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 70/478 | 14.64%\n",
      "[0:13:46.763805<1:18:59.336327, 0:00:11.644561s/generations] fine-tuned-mistral-7b-v0.2-instruct - promt 2: 71/478 | 14.85%\n"
     ]
    }
   ],
   "source": [
    "all_runs = pd.DataFrame()\n",
    "for i in range(len(SYSTEM_PROMPT)):\n",
    "    run = run_model(i, \"fine-tuned-mistral-7b-v0.2-instruct\")\n",
    "    all_runs = pd.concat([all_runs, run])\n",
    "    pd.DataFrame(all_runs).to_pickle(\"fine-tuned.pkl\")"
   ],
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-11T04:01:34.727708312Z"
    }
   },
   "id": "05aa5115",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3223579e746ec60",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
